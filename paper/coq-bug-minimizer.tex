\documentclass[a4paper,UKenglish,anonymous,cleveref,autoref,thm-restate]{lipics-v2021}
%This is a template for producing LIPIcs articles.
%See lipics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Dummy title} %TODO Please add

%\titlerunning{Dummy short title} %TODO optional, please use if title is longer than one line

\author{Jason Gross}{CSAIL, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139, USA \and MIRI, USA \and \url{https://jasongross.github.io/} }{jgross@mit.edu}{https://orcid.org/0000-0002-9427-4891}{}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional. Use additional curly braces to indicate the correct name splitting when the last name consists of multiple name parts.

\author{Théo Zimmermann}{Department of Informatics, Dummy College, [optional: Address], Country}{joanrpublic@dummycollege.org}{[orcid]}{[funding]}

\author{Adam Chlipala}{CSAIL, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139, USA \and \url{http://adam.chlipala.net/} }{adamc@csail.mit.edu}{[orcid]}{[funding]}

\authorrunning{J. Gross and T. Zimmermann and A. Chlipala} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Jason Gross and Théo Zimmermann and Adam Chlipala} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\ccsdesc[100]{\textcolor{red}{Replace ccsdesc macro with valid one}} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm

\keywords{Dummy keyword} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatletter
\newcommand{\todo}[1]{%
\@latex@warning{TODO: \detokenize{#1} on page \thepage}%
\textcolor{red}{[\textbf{TODO:} #1]}}%
\makeatother

\begin{document}


\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
Text of abstract \ldots.
\end{abstract}


\section{Introduction}

Text of paper \ldots

\section{Improving the coverage of Coq's test suite}

\subsection{Context and objectives}

Shipping new releases of a compiler frequently can be a challenge
because catching regressions is not trivial, even with an extensive
test suite.
%
This is why, the Rust compiler, for instance, is tested not only on a
large test suite, but also on all the external Rust source code
available on GitHub.\footnote{As narrated by Pietro Albini in his talk
``Shipping a compiler every six weeks'', available at
\url{https://www.youtube.com/watch?v=As1gXp5kX1M} and whose transcript
is available at
\url{https://www.pietroalbini.org/blog/shipping-a-compiler-every-six-weeks/}.}

In the case of the Coq proof assistant, the development team decided
to shorten the release cycles not to six weeks but to six months, but
this still represented a significant challenge to do so reliably.
%
Therefore, in early 2017, they adopted a ``reverse'' continuous
integration, i.e., a continous integration configuration including a
large portion of compatibility testing with external Coq projects (Coq
projects maintained by different teams in different
repositories)~\cite{zimmermann:tel-02451322}.

This continuous testing of external Coq projects represents in itself
a continuous challenge because of the specificities of Coq's
evolution.
%
Indeed, as of today, Coq's evolution still includes many breaking
changes in each release (breaking changes that are evaluated,
validated and documented, and therefore cannot be classified as
regressions, but still break some of the tested projects).
%
Every time such breaking changes are introduced, Coq developers must
fix the tested projects for compatibility, because they would not be
usable for testing anymore if that was not done.
%
Furthermore, fixing external projects can help Coq developers assess
in more depth the practical impact of a breaking change.

A rough assessment shows that more than half of the time that a change
to Coq breaks at least one external tested project, the Coq test suite
still passes fully~\cite{zimmermann:tel-02451322}.
%
We interpret this as insufficient coverage of the Coq test suite with
respect to the features that are actually relied on by users.
%
Therefore, a primary objective of this work was to help improve the
coverage of the test suite by systematically extracting a minimal
reproducible example from any compatibility issue detected while
testing a change to Coq.

A secondary objective was to help Coq developers assess more easily
the kind of compatibility issues that they introduce with their
changes.
%
Indeed, every change to Coq is tested against millions of lines of
externally maintained Coq code.
%
Some of the tested projects can be difficult to navigate and
understand for Coq developers.
%
Therefore, extracting a minimal reproducible example of a
compatibility issue can help the developer to determine whether it is
a regression or a desirable change and therefore to decide whether to
fix the change to Coq or the tested project for compatibility.

\subsection{Technology}

\subsubsection{Extending the bot of the Coq development team with a CI minimization feature}

\subsubsection{Adapting the bug minimizer for minimizing compatibility issues}

\subsubsection{Using GitHub Actions to run the bot}

\subsection{Evaluation}

\section{How the bug minimizer works}
Broadly, the bug minimizer is an adaptation of delta-debugging for Coq developments.

\subsection{The Spec}\label{sec:spec}
The bug minimizer runs a given development through Coq and parses the error message.
The goal is to construct a smaller, more self-contained script that generates the same error message.
An additional constraint that we place on the minimizer is that the proof script generating the error message should be left untouched.
This allows users to write proof scripts such as
\begin{verbatim}
some_tactic;
let G := match goal with |- ?G => G end in
lazymatch G with
| buggy_goal => fail 0 "bug remains"
| _ => fail 0 "bug disappeared!" G
end.
\end{verbatim}
to customize the desired reproducing case, and the minimizer will not decide that it can minimize the entire file to something silly like \verb|Goal False. fail 0 "bug remains"|.

For CI minimization, we furthermore want to preserve the lack of error message on one version of Coq while preserving the error message on the more recent, proposed version of Coq.

\subsection{Broad Insights}
There are two broadly interesting insights used to make delta-debugging run well with Coq:
\begin{enumerate}
\item
  Since Coq forbids forward-reference, removing earlier statements almost never allows the removal of later statements.
  Hence we can mostly just make a linear pass over statements in reverse order, attempting to remove one at a time.
\item
  In our goal to get the shortest reproducing test case as quickly as possible, it behooves us to first make any changes that might significantly speed up the execution time, and only after we're done with all of the changes that might improve running time should we try to further minimize the file with changes that are unlikely to impact compile time.
  The slowest part of almost all Coq developments is proof scripts.
  Hence it behooves us to attempt to remove proof scripts as early as possible.
  Coq has a couple of mechanisms for removing proof scripts without removing the theorems and definitions being proven (the commands \verb|Admitted| and \verb|Admit Obligations| as well as the \verb|admit| tactic).
  Replacing proof blocks with these commands, rather than just removing proof scripts, allows us to make much smaller and faster examples than might otherwise be possible.
\end{enumerate}

In addition to this, there are four fundamentally different sorts of actions the bug minimizer needs to perform, all of which have their own laundry list of corner cases which drive various adaptations in the minimizer to handle these cases.
The sorts of actions are:
\begin{enumerate}
\item
  Determine the failing file, the error message, and the command-line arguments necessary to reproduce the error message.
\item
  Determine what it means for the error message to ``be preserved'' (we do not want to require, for example, that source-code line number attached to the error message is always the same).
\item
  Perform source code transformations on the failing code while preserving the error message, either replacing code with other code (as when admitting proof scripts) or removing statements.
\item
  Inline dependencies of the file, making it more stand-alone.
\end{enumerate}

\subsection{Control Flow}
Bug minimizer execution has a couple of steps.
We include here the steps that are performed by external scripts for CI minimization.

Steps before the bug minimizer is invoked:
\begin{enumerate}
\item Unpack and install both the succeeding and failing versions of Coq and corresponding developments.
\item Replace the Coq binaries with wrappers that print out the arguments that Coq was called with, as well as COQPATH and pwd.
  \todo{Maybe add text somewhere that I (Jason) think this is a pretty cool way of not having to deal with varied build systems of CI developments.
    Note that there are a bunch of subtleties here: if we don't get access to COQPATH, we won't be able to build some developments.
    If we don't print out the pwd, we can find the right bug file by absolutizing the arguments to coq, but this breaks output tests, cf https://github.com/math-comp/hierarchy-builder/issues/256.}
\item Run Coq on the succeeding and failing developments, ensuring that the version that should pass does in fact pass, and the version that should fail has a recognizable error message.
\item Parse the build log to determine the buggy file name and the arguments to pass to Coq.
  This workflow means that we don't need to directly interface with the varied build systems of various contributions on the CI.
\end{enumerate}

Control flow of the bug minimizer:
\begin{enumerate}
\item Run Coq on the buggy file.
\item Parse the error message, ensuring that it matches with the error message from the build log.
  (Note that there are a bunch of error message changes that we allow, such as changing evar numbers and universe numbers, etc.)
\item Repeat the following steps until a fixed point is reached, skipping any steps that change the error message in a forbidden way.
\begin{enumerate}
\item Remove comments, inserting a header comment with metadata about minimization statistics.
\item Insert a further header which defines the \verb|admit| tactic.
  We must define this tactic ourselves to handle files which build with a non-standard standard library (such as HoTT) and hence don't have access to \verb|Coq.Compat.AdmitAxiom|.
  To support multiple versions of Coq, we need to feature-test Coq to see what fragment of code grants us access to Ltac without loading the standard library; this feature-test is done once and for all for a single run of the bug minimizer.
\item Split \verb|Require| statements from \verb|Import|/\verb|Export|, disambiguating the statements at the same time.
  (If we don't disambiguate them, the \verb|Import|s might point to the wrong module.)
\item Move the \verb|Require| statements to the top of the file, transitively closing them (this allows us to remove \verb|Require| statements which are only needed to transitively require other files).
\item Parse the file into statements (with exactly two exceptions, statements in Coq end with \verb|.| or \verb|...| followed by whitespace; the exceptions are \verb|{| and \verb|}|).
  Note that this is complicated by needing to handle strings which contain a full stop followed by whitespace, and by the fact that comments in Coq are well-balanced and nestable and can contain strings which themselves contain comment identifiers without nesting further comments.
\item Remove statements after the line which generates the error.
\item Split the statements into structured blocks, one per definition/proof.
  We should not assume that we can remove lines from the end of a proof without breaking things, so we treat each theorem/lemma/definition block, combined with its proof, as a unit.
  This is made simpler by the fact that Coq strongly discourages nested proofs, and by the fact that Coq generally forces definitions and their bodies to be side-by-side.
  There are a couple of exceptions to this strategy, that have dedicated passes for handling them:
  \begin{itemize}
  \item \verb|Program| \verb|Definition|s, \verb|Fixpoint|s, etc, have \verb|Obligation| blocks which can be interwoven with other definitions.
    The solution here is to replace such \verb|Obligation| blocks with \verb|Admit Obligations|, which is thankfully a no-op when no obligations are present.
    \todo{Insert some code showing how minimization of program definitions proceeds with admit obligations}
  \item Sections, modules, and module types have paired vernacular delimiters.
    Currently there is some code to eliminate empty blocks, but it doesn't work very well, and misses many cases.
    There's an open feature request for a \verb|Try| vernacular command at https://github.com/coq/coq/issues/15051, which would solve the issue of coupled commands in general.
  \end{itemize}
\item Repeat the following steps until a fixed point is reached (order presented here is not faithful to the minimizer):
  \begin{itemize}
  \item Remove each structured block (whose removal does not change the error message), one at a time, from the end of the file to the beginning of the file.
  \item Replace proof scripts with \verb|Admitted|.
    (There's some nuance about the best ordering strategy to try here, whether to first try all the \verb|Qed| lemmas, whether to first try all lemmas and definitions at once, or whether to just go one at a time in reverse order.)
    Also, we actually replace them with \verb|admit. Defined.| so that previously unfoldable constants remain unfoldable.
    Technically we should try \verb|Admitted| when the proof script previously ended with \verb|Qed| and replacing with \verb|admit. Defined.| fails, because it might be the case that some later tactic relies on this definition not being unfoldable, but we have not encountered such a case yet, and it seems quite unlikely.
  \item Replace \verb|abstract tac| with \verb|admit|, potentially simplifying proof scripts and decreasing dependencies.
  \item Split \verb|Definition foo args : ty := body.| into \verb|Definition foo args : ty.| and \verb|Proof. refine (body). Defined.| so that such definitions can potentially be admitted later.
  \item Replace \verb|Module Foo| with \verb|Module Export Foo|, potentially allowing the removal of \verb|Import| statements later, and potentially eventually allowing the removal of the module itself.
  \item Split \verb|Import| and \verb|Export| statements containing multiple modules into separate statements, so they can be removed separately.
  \item Early on, some likely-to-succeed steps are tried, such as removing tactics, \verb|Variable| and \verb|Context| statements, and definitions which are not referred to at all after their definition.
    This step is superseded by removing each and every structured block one at a time, but may result in faster minimization.
  \end{itemize}
\item Finally, before repeating the loop, attempt to inline a dependency which has not yet been inlined.
  Inlining dependencies is an interesting technical challenge, due to nuances in how Coq binds names and handles global state.
  \todo{write more text about what's required here, and what still remains}
\end{enumerate}
\end{enumerate}

\subsection{Walkthrough via Examples}

\subsubsection{Infrastructure around Invoking the Minimizer}
As described in \autoref{sec:spec}, the goal of the minimizer is to take a CI development that succeds on the tip of master and fails on a given PR, and emit a small, stand-alone file which succeeds on master and fails in the same way \todo{same modulo universes, etc, stick this somewhere} on the PR.
\todo{Should we cite \url{https://github.com/dsw/delta\#do-a-controlled-experiment} about passing coqc, a la ``Below we don't just minimize a file that causes Oink to produce an error message, we minimize a file that causes gcc to accept AND oink to reject in a specific way. That is, the test delta does is a controlled experiment, where gcc is the control. Ignoring this aspect of the problem seems to be a frequent mistake of first time users.''}

In order to do this efficiently, we re-use the CI artifacts from Coq.
We download the pre-built versions of Coq from master and from the tip of the PR.
From just these artifacts and the name of the failing CI development, we must assemble enough information to run the bug minimizer.
We replicate Coq's generic CI workflow to install Coq as well as any dependencies of this CI development, into different directories: one for the version of Coq that's supposed to pass, and another for the version of Coq that's supposed to fail.
We also reuse Coq's generic CI workflow to figure out the error message and the failing file which we want to minimize.
Much like \todo{cite https://github.com/dsw/build-interceptor}, rather than trying to parse the build setup of various CI developments, we merely wrap the Coq binaries to intercept the build-system calls, and print out the relevant information to stderr.
\todo{Add a note about future work for redirecting things to a log file from within the wrapper, so that we don't change the build output at all, which would, perhaps, allow us to minimize coq-tools itself, maybe}
The relevant information in this case, is the current working directory of coqc, the environment variable \texttt{COQPATH}, the command line used to invoke the binary, and the relative path of the file that coqc is invoked on.
We need \texttt{COQPATH} to ensure that we have the right search path for the dependencies of coqc.
\todo{maybe explain COQPATH somewhere?}
We need the command line arguments so that we know what flags to tell the bug minimizer to pass to coqc.
We could in theory elide the file name and the current working directory, as long as we changed all relative paths to absolute paths before printing the command line.
(Unless the file uses directives like \texttt{Cd} or \texttt{Add LoadPath} with relative paths.
However, we don't currently test nor support these cases directly.
\todo{should we include this paranthetical?})
Note that we \emph{must not} change relative paths to absolute ones when passing arguments along to coqc, because the output of coqc is sensitive to the difference between relative and absolute paths, and this can mess up output tests (and did in the past, for example with ci-elpi).
We can locate the error message by looking for the last instance of \texttt{File "$f$", line $\ell$, characters $n$-$m$:} followed immediately by a line beginning with \texttt{Error}.
(Note that warning messages also emit the \texttt{File $\ldots$} line, but we don't want to catch warnings.)
We look for the last instance of the wrapper debug printout information which points at the same file, though, so long as we were careful to always build single-threadedly, we could instead just look for the most recent debug printout before the error message.

Given this information, we adjust the arguments so that we can tell the bug minimizer where the dependencies live both for the passing and failing versions of Coq.
We then pass this information to the bug minimizer:
\begin{itemize}
\item the location of the file to be minimized;
\item the log file containing the error message, which must match the error message that the minimizer believes the file produces;
\item the location of the \verb|coqc|, \verb|coqtop|, and \verb|coq_makefile| for the tip of the PR
\item the location of the \verb|coqc| for the master branch
\item the location of the dependencies for both the passing and failing versions of Coq, parsed from the command line arguments and from walking the directories in \texttt{COQPATH}
\item any arguments to \verb|coqc| which are neither naming dependency locations nor known to be both irrelevant to the processing of the file and counter-productive to the minimizer's operation (such arguments are \verb|-batch| which applies only to \verb|coqtop|, \verb|-time| which will only make logs of the minimizer much longer, and \verb|-noglob|, \verb|-dump-glob|, and \verb|-o|, which interfere with the generation of outputs used by the minimizer).
\end{itemize}

Henceforce, we will assume that we have all of this information at hand, and speak only of how to minimize files.

\subsubsection{The Basics}
The simplest function of the bug minimizer is to remove unneeded lines.
For example, if a Coq PR breaks the \texttt{rewrite} tactic (an admittedly simplistic example in this generality), we might want to minimize a file such as
\begin{verbatim}
Definition zero := 0.
Definition one := 1.
Definition two := 2.
Lemma foo : forall x, x = zero -> S x = one.
Proof. intros x H. rewrite H. reflexivity. Qed.
\end{verbatim}
into
\begin{verbatim}
Definition zero := 0.
Definition one := 1.
Lemma foo : forall x, x = zero -> S x = one.
Proof. intros x H. rewrite H.
\end{verbatim}
Note that the definition \texttt{two} has been removed.

Here it suffices to (a) remove all statements after the error, and (b) try removing each line one at a time.
Since Coq is mostly not sensitive to whitespace, we want to remove statements, rather than lines.

We may someday depend on a proper Coq parser (such as SerAPI), but we currently do in-house parsing.
Even if we move to depending on a proper parser, we will sometimes have to do transformations on files where we cannot do full parsing.
See \autoref{sec:glob-for-inline-installed-files-without-parsing} for more details.
\todo{make sure \autoref{sec:glob-for-inline-installed-files-without-parsing} exists}
Furthermore, we continue to support versions of Coq all the way back to 8.5, in part because minimization is also useful for porting older Coq developments to newer versions of Coq, and so any plugin-based parsing will not fully subsume the existing parsing methods.

We currently have three methods of parsing Coq documents:
\begin{enumerate}
\item Parsing implemented in-house in Python.
\item Using \texttt{coqc -time} to get byte ranges for each statement.
  Note that these are \emph{byte} ranges and not character ranges, so we have to be careful when documents contain unicode.
\item Using \texttt{coqtop -emacs -time}, which, in addition to providing byte ranges for statements, also provides information about which lines are part of which definitions (via the emacs prompt), and which lines close definitions (via the \texttt{\emph{foo} is defined} message).
  Note that \verb|-emacs| is technically meant for legacy interaction with the ProofGeneral IDE, so we may someday have to move to a different API.
  Hopefully by the time \verb|-emacs| is removed, we'll be ready to use plugin-based parsing via SerAPI or similar.
\end{enumerate}

Because Coq forbids forward references (definitions cannot refer to any identifiers that have not yet been added to the global environment, and documents are processed serially), we can remove all lines after the line where the error occurs without risking a change in error messages.
Note that the error message line indicator is not always accurate; the recent bug at \url{https://github.com/coq/coq/issues/15073}\todo{formatting of link} was an instance where the error location was mis-reported.

This lack of forward referencing also means that we generally don't need to do any sort of non-local search on removing statements.
Since removing an earlier statement almost never enables removing a later statement, we can generally just try removing statements one at a time, in reverse order.

Consider now a file such as.
\begin{verbatim}
Definition zero := 0.
Definition one := 1.
Definition two := 2.
Lemma irrelevant : two = 2.
Proof. reflexivity. Qed.
Lemma foo : forall x, x = zero -> S x = one.
Proof. intros x H. rewrite H. reflexivity. Qed.
\end{verbatim}
Since Coq forbids nested lemmas, removing statements one at a time will not work, as the state
\begin{verbatim}
Definition zero := 0.
Definition one := 1.
Lemma irrelevant : two = 2.
Proof. reflexivity.
Lemma foo : forall x, x = zero -> S x = one.
Proof. intros x H. rewrite H.
\end{verbatim}
results in an error about nested proofs.

We instead group statements into \emph{definition} blocks to be removed all at once.
We get information about definitions by parsing the output of \texttt{coqtop -emacs -time}, as mentioned above.
This way, we can remove
\begin{verbatim}
Lemma irrelevant : two = 2.
Proof. reflexivity. Qed.
\end{verbatim}
all at once.

We could in theory deal with more complicated nesting structure, for example trying to remove an entire section or module at a time.
The tool at \url{https://github.com/dsw/delta#exploit-nested-structure} \todo{better citation} is in fact built around preprocessing the file into one that exposes nested structure clearly, and then removing well-parenthesized blocks.
However, removing statements, grouped into definitions as necessary, sufficies for removing time-consuming code.
(\todo{Find a better place for this theme, call it out more broadly:}%
In general, it's very important to remove any lines that can be removed that take significant compilation time, and much less important to remove lines that take basically no compilation time.
\todo{what more should be said about this?}
)

\paragraph{Admitting Proofs}
Another enormously useful transformation is \emph{admitting proofs}.
For example, suppose that some PR broke the ability of \verb|rewrite| to make use of lemmas proving equalities between propositions.
Consider the file:
\begin{verbatim}
Lemma complicated : True = False.
Proof.
  (* ... *)
  (* thousands of lines of proof
     taking minutes to compile *)
  (* ... *)
Qed.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}
We cannot remove statements here, but we can minimize this file to
\begin{verbatim}
Lemma complicated : True = False.
Admitted.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

This is one of the domain-specific features of Coq that makes delta-debugging a significantly different problem than in other domains:
There are slow blocks of code that cannot be entirely removed without changing the error message, but they can be replaced by a much faster catch-all result.

There are some quirks to be aware of around this.

The first quirk is around unfolding.

Consider the file:
\begin{verbatim}
Lemma complicated : True = False.
Proof.
  (* ... *)
  (* thousands of lines of proof
     taking minutes to compile *)
  (* ... *)
Defined.
Lemma boom : True -> False.
Proof.
  pose complicated as H.
  unfold complicated in H.
  rewrite H.
\end{verbatim}
If we replace the proof script of \verb|complicated| with just \verb|Admitted.|, then the line \verb|unfold complicated in H| will fail with \verb|Error: complicated is opaque|.
We can work around this issue by copying the \verb|admit| tactic from \texttt{Coq.Compat.AdmitAxiom}, which is effectively a tactic version of \verb|Admitted|:
\begin{verbatim}
Axiom proof_admitted : False.
Ltac admit := clear; abstract case proof_admitted.
\end{verbatim}
Now we can replace the long proof of \verb|complicated| with \verb|admit. Defined.|

Conversely, we might have a file where the error message relies on \emph{not} being able to unfold a lemma whose proof script ended with \verb|Qed|.
(For a very silly example, consider prefixing an essential tactic with \verb+unfold complicated ||+.)
Hence we must support both \verb|Admitted| and \verb|admit. Defined.| as replacements.

While we could just \verb|Require Import Coq.Compat.AdmitAxiom| to get this \verb|admit| tactic, there are two cases where this would not work:
(1) in libraries such as HoTT and UniMath, where the standard library is not loaded, we cannot assume that \texttt{Coq.Compat.AdmitAxiom} will be found;
(2) in versions of Coq prior to 8.5%
\footnote{%
We supported Coq 8.4 until Ubuntu 12.04 LTS (Precise Pangolin) was no longer supported on CI servers.
Coq 8.4 required OCaml < 4.05 to build, due to a change in OCaml string delimiters in comments.
Ubuntu 12.04 was the last version of Ubuntu to have OCaml < 4.05.
We're currently in the process of re-adding support for Coq 8.4, though, via the docker-coq GitHub action.%
}%
, the \verb|AdmitAxiom| file did not exist (instead, the \verb|admit| tactic available by default performed this function).
Hence we provide our own tactic, rather than importing one from the standard library.

\todo{this subsubsection deserves a better structure, I think\ldots}

Additionally, we may want to admit some parts of the proof script without replacing all of it.
Currently, we use a rather conservative heuristic:
Coq has a tactical \verb|abstract| which executes the tactic it is passed as an argument, and makes the resulting proof term opaque.
Such subproofs should be able to be replaced with \verb|admit| without changing the behavior of the proof script.
To make this change more likely to succeed, we define \verb|admit| as \verb|abstract case proof_admitted| rather than \texttt{clear; abstract case proof\_admitted}.

Here is a case where this matters:
Consider the file
\begin{verbatim}
Section foo.
  Context (a : nat).
  Definition foo : nat.
  Proof.
    refine (_ + _).
    all: abstract exact a.
  Defined.
End foo.
Goal True.
  pose (foo 5) as x.
  lazymatch (eval unfold x, foo in x) with
  | ?x + ?y => idtac
  | ?v => fail 0 "bad" v
  end.
\end{verbatim}
If we replace \verb|abstract exact a| with \texttt{clear; abstract case proof\_admitted}, then the definition \verb|foo| no longer depends on \verb|a|, and the \verb|pose| line will fail with a type error.

In the future, when we do more fine-grained parsing of the Coq document structure, we may want to try replacing more subparts of proof scripts with \verb|admit|.
See \autoref{sec:future-work} for more details.

There are three more corner-cases which result in us using slightly different code in practice.

First, because libraries like HoTT don't load the standard library by default, we need to declare our own \verb|False| inductive and we must also load support for Ltac.
In all versions of Coq prior to 8.12, \verb|Require Import Coq.Init.Notations| was adequate to get Ltac.
In Coq 8.12 and later, we need to either \verb|Require Import Coq.Init.Ltac| (which did not exist in prior versions of Coq) or \verb|Declare ML Module "ltac_plugin"|.
We choose the latter of these two, for compatibility with developments like HoTT which may not have a \verb|Coq.Init.Ltac| file available.

Note that we need to issue \verb|Inductive False : Prop := .| rather than \verb|Inductive False := .|, because any developments that pass \verb|-set "Universe Polymorphism"| on the command-line will otherwise end up with a polymorphic \verb|False| inductive which will introduce extra universes when using the \verb|admit| tactic which may break some files.
(This was encountered in the wild; see \url{https://github.com/JasonGross/coq-tools/issues/74} for details.\todo{better urls?})

Second, because the file we're minimizing may make use of \verb|False| and may rely on it being syntactically equal to the version in Coq's standard library \todo{does this need an example?}, we must wrap our \verb|Inductive False| in a module that is not exported to the file.
This means that the \verb|admit| tactic must itself be in a module that is exported.
We could alternatively qualify the version of \verb|False| that we're using, but this would prevent removal of our \verb|Inductive False|, and we would prefer to be able to remove this line when the Coq standard library is available.
The code we now have, after the Ltac support snippet, is:
\begin{verbatim}
Module Export AdmitTactic.
Module Import LocalFalse.
Inductive False : Prop := .
End LocalFalse.
Axiom proof_admitted : False.
Ltac admit := abstract case proof_admitted.
End AdmitTactic.
\end{verbatim}

Third, and finally, the line \texttt{Ltac admit := abstract case proof\_admitted} errors in Coq 8.4 with ``Error: Reserved Ltac name admit.''
Hence we use \texttt{Tactic Notation "admit" := abstract case proof\_admitted} instead.

Thus the fragment we add for the \verb|admit| tactic is ultimately:
\begin{verbatim}
Module Export AdmitTactic.
Module Import LocalFalse.
Inductive False : Prop := .
End LocalFalse.
Axiom proof_admitted : False.
Tactic Notation "admit"
  := abstract case proof_admitted.
End AdmitTactic.
\end{verbatim}

\subsubsection{Coupled Removal}

\paragraph{Program Mode}
We saw above that proof scripts could not be removed one statement at a time, but instead had to be transformed all at once.
Coq has a \verb|Program| mode which results in even more coupling: removing one proof script at a time is not enough to remove the \verb|Program Definition|.

Consider, for example:
\begin{verbatim}
Require Import Coq.Program.Tactics.
Section __.
Context (arg : True).
Lemma complicated (pf : arg = arg) : True = False.
Admitted.
Axiom bad : False.
Definition another_False := False.
Program Definition irrelevant
  : another_False /\ another_False := conj _ _.
Next Obligation. exact bad. Qed.
Next Obligation. exact bad. Qed.
End __.
Lemma boom : True -> False.
Proof. rewrite (complicated I eq_refl).
\end{verbatim}
We can easily minimize this into
\begin{verbatim}
Require Import Coq.Program.Tactics.
Section __.
Context (arg : True).
Lemma complicated (pf : arg = arg) : True = False.
Admitted.
Definition another_False := False.
Program Definition irrelevant
  : another_False /\ another_False := conj _ _.
Next Obligation. Admitted.
Next Obligation. Admitted.
End __.
Lemma boom : True -> False.
Proof. rewrite (complicated I eq_refl).
\end{verbatim}
However, at this point, we cannot remove anything else.

If we remove \verb|End __|, we get an error message indicating that \verb|complicated| expects a proof of type \verb|arg = arg|, not two arguments, one of type \verb|True| and the second a proof that the first equals itself.
If we remove either of the \verb|Next Obligation. Admitted.| blocks, we get an error message indicating that we cannot close a section with unsolved obligations.
If we remove the \verb|Program Definition irrelevant| line, we get an error indicating that there are no obligations to be solved.

We cannot just look for \verb|Program| statements followed by \verb|Obligation| blocks to remove, because \verb|Obligation| blocks can be interleaved with other definitions.

Luckily, we can replace \verb|Next Obligation. Admitted.| with \verb|Admit Obligations|.
Since \verb|Admit Obligations| is luckily a noop when there are no obligations to admit, we can do this to both \verb|Next Obligation. Admitted.| blocks.
After performing this transformation, we can remove the \verb|Program Definition| line (without needing to remove the \verb|Admit Obligations| lines), and thereafter minimize the file to
\begin{verbatim}
Section __.
Context (arg : True).
Lemma complicated (pf : arg = arg) : True = False.
Admitted.
End __.
Lemma boom : True -> False.
Proof. rewrite (complicated I eq_refl).
\end{verbatim}

\paragraph{Sections}
Removing statements one at a time will not always be able to remove empty sections (nor empty \verb|Module|s or \verb|Module Type|s).

For example, consider the file
\begin{verbatim}
Section useless.
End useless.
Module useful.
Lemma complicated : True = False.
Admitted.
End useful.
Lemma boom : True -> False.
Proof. rewrite useful.complicated.
\end{verbatim}

This file can be minimized to
\begin{verbatim}
Section useless.
End useless.
Module useful.
Lemma complicated : True = False.
Admitted.
Lemma boom : True -> False.
Proof. rewrite useful.complicated.
\end{verbatim}
because Coq allows refering to constants in the currently-open module via qualified names.

However, it cannot be minimized further, because if we remove \verb|End useless|, then we get an error message indicating that modules are forbidden inside sections.

Hence we have a pass dedicated to removing empty sections, modules, and module types.

\paragraph{Exporting Modules}
Consider the file
\begin{verbatim}
Module foo.
Lemma complicated : True = False.
Admitted.
End foo.
Import foo.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}
We cannot remove \verb|Import foo| because then \verb|complicated| can't be found.
But we cannot remove the \verb|End foo| because then \verb|foo| can't be found when trying to \verb|Import foo|.

The solution to this coupling is to replace \verb|Module foo| with \verb|Module Import foo| or \verb|Module Export foo|, which simultaneously opens the module and declares that it is to be imported (or exported) after then \verb|End| statement.
This allows us to remove \verb|Import foo| (because \verb|foo| is already imported), and after that we can remove \verb|End foo| and then \verb|Module foo|, allowing us to minimize this file.

We choose to do \verb|Export| rather than \verb|Import| because it allows for removal of some \verb|Import| statements even when chaining exports.
Consider, for example, the file
\begin{verbatim}
Module A.
  Module A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module A.
    Axiom A : Set.
  End A.
End B.
Module A1. Export A. End A1.
Module B1. Export B. End B1.
Module Import A2. Import A1. Import A.
 Definition A2 := A. End A2.
Module Import B2. Import B1. Import A.
 Definition B2 := A. End B2.
Check eq_refl : A2 = B2.
\end{verbatim}
If we use \verb|Export|, then we can turn this file into
\begin{verbatim}
Module A.
  Module Export A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module Export A.
    Axiom A : Set.
  End A.
End B.
Module Export A1. Export A. End A1.
Module Export B1. Export B. End B1.
Module Import A2. Import A1. Import A.
 Definition A2 := A. End A2.
Module Import B2. Import B1. Import A.
 Definition B2 := A. End B2.
Check eq_refl : A2 = B2.
\end{verbatim}
whence we can minimize it to
\begin{verbatim}
Module Export A.
  Axiom A : Set.
End A.
Module B.
  Module Export A.
    Axiom A : Set.
  End A.
End B.
Module Export A1. Export A. End A1.
Export B.
Module Import A2. Import A1.
 Definition A2 := A. End A2.
Definition B2 := A.
Check eq_refl : A2 = B2.
\end{verbatim}
However, if we had used \verb|Import| instead, we get the file
\begin{verbatim}
Module A.
  Module Import A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module Import A.
    Axiom A : Set.
  End A.
End B.
Module Import A1. Export A. End A1.
Module Import B1. Export B. End B1.
Module Import A2. Import A1.
 Import A. Definition A2 := A. End A2.
Module Import B2. Import B1.
 Import A. Definition B2 := A. End B2.
Check eq_refl : A2 = B2.
\end{verbatim}
which can only be minimized to
\begin{verbatim}
Module A.
  Module Import A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module Import A.
    Axiom A : Set.
  End A.
End B.
Module Import A1. Export A. End A1.
Export B.
Module Import A2. Import A1. Import A.
 Definition A2 := A. End A2.
Import A. Definition B2 := A.
Check eq_refl : A2 = B2.
\end{verbatim}
Granted, the additional removal of \verb|Import A| and one of the \verb|Module A| \ldots\ \verb|End A| pairs is not all that impressive.
On the other hand, using \verb|Export| instead of \verb|Import| costs nothing, and we must choose one of them.

\subsubsection{More Strategies for Minimizing Files}

\paragraph{Splitting Definitions}
Coq permits definitions to give the body at the same time as the type.
Consider, for example, the file
\begin{verbatim}
Definition complicated : True = False
  := (* thousands of lines of proof term
        taking minutes to compile *).
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}
We would like to be able to admit this proof term, too.
To do this, there is a pass in the minimizer that attempts to split apart such definitions into
\begin{verbatim}
Definition complicated : True = False.
refine (
     (* thousands of lines of proof term
        taking minutes to compile *)
  ).
Defined.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}
to enable subsequent admitting of proof scripts.

This pass relies on the \verb|refine| tactic, which allows users to give a proof term in proof mode.

\paragraph{Early Removal of Unused Constants}
There are some likely-to-succeed steps that we try early on, which are superseded by removing each and every structured block one at a time, but may result in faster minimization.
The primary example of this sort of step is removing tactics, \verb|Variable| and \verb|Context| statements, and definitions which are not referred to at all after their definition.

\paragraph{Splitting \texttt{Import} and \texttt{Export}s}
It may be the case that users import modules that they never use, such as in \texttt{Import unused1 used unused2}.
To allow eventual removal of \verb|unused1| and \verb|unused2| even when the \verb|Import used| statement cannot be removed, we have a pass that attempts to split such statements into separate \verb|Import| statements, resulting in \texttt{Import unused1. Import used. Import unused2.}

\paragraph{Splitting \texttt{Require}s}
Much like \verb|Import| and \verb|Export| statements, we also want to split \verb|Require| statements, and furthermore want to split \verb|Require Import foo| into \texttt{Require foo. Import foo.}
This last splitting will allow us to inline \verb|foo|, as described in \autoref{sec:inline-deps}.
However, there are three subtleties associated with such statements.

First, it may be the case that our bug depends on \verb|Require foo| only because the file \verb|foo.v| contains \verb|Require bar|, and in fact the bug only needs access to the constants defined in \verb|bar.v|.
Hence we have a pass that performs the transitive recursive closure of the dependency relation, and inserts \verb|Require| statements at the top of the file for all transitive dependencies of the file being minimized.
Note that if we were more careful, we would not insert all dependencies at the top of the file, but instead at each \verb|Require| statement, because \verb|Require| has side-effects that may cause the file to fail to compile; we'll discuss this more in \autoref{sec:inline-deps}.
Because we insert the \verb|Require|s in dependency order, removing one statement at a time in reverse order will give us the minimal \verb|Require|s needed to reproduce the error message.
\todo{Does this need more explanation?}

Second, it may be the case that \verb|Require Import foo bar baz| is not the same as \texttt{Require foo. Require bar. Import foo. Import bar.}.
It may be the case that \verb|Import foo| results in a different module \verb|bar| becoming available.
Alternatively, it may be the case that \verb|Require bar| transitively requires another \verb|foo| file which is different than the one that that is resolved by default as \verb|Require foo|, and then \verb|Import foo| will choose the most recently \verb|Require|d module \verb|foo|.
\todo{Does this require a more detailed, worked-out example?}

The solution to this problem relies on the fact that \verb|coqc| emits \emph{globalization} files, which contain information about where bits of source code point.
These \verb|.glob| files are primarily used to create hyperlinked html versions of \verb|.v| files, but we can co-opt them to get the information we need.
By generating a \verb|.glob| file for our buggy file and reading it, we can determine the absolute paths of all \verb|Require|, \verb|Import|, and \verb|Export| statements, allowing us to split them as needed without worrying about changes to name resolution.

Third and finally, Coq supports statements such as \verb|From Coq Require Import Arith|.
Na\"ively replacing this with \verb|From Coq Require Import Coq.Arith.Arith| results in an error.
Hence, after absolutizing module paths for a given statement, we carefully remove the \texttt{From \ldots} portion of the statement.

\subsubsection{When are two error messages ``the same''?}
The following are ways that two error messages can differ while still being considered ``the same''.

\paragraph{Universe inconsistency explanations}
Universe inconsistencies are how Coq prevents users from proving absurdity by assuming a ``set of all sets''.
The explanations of universe inconsistencies in error messages are generally very sensitive to how many universes are floating around and what order constraints were added.
Rather than requiring that the output file mimic the error message exactly, we merely require that it result in \emph{some} universe inconsistency.

\paragraph{Unbound universes}
Unbound universes are generally internal errors in Coq involving updating the environment with new universes and then forgetting about the update.
We never care which particular universe has been forgotten, merely that some universe was forgotten.

\paragraph{Numbering}
Numbering in error messages is generally pretty sensitive; the most common source of sensitivity is universe numbering, though differences in what constants are present can cause autogenerated names to use different numbers.
The one case where we do care about numbering is if the error message is about universe instance lengths, where we want to find out not just that the length mismatched, but what caused it to have the particular mismatch.
(Otherwise it might be valid to reduce to the case where we just remove all the universes from the given constant, which is not what we want.)
If the error message does not contain ``Universe instance should have length'', then we skip over any differences in numbers.

\paragraph{Unsatisfied constraints}
Coq will very rarely emit an error message indicating that it has forgotten to satisfy some unification constraints with a message of the form ``Unsatisfied constraints: \ldots\ (maybe a bugged tactic)''.
We don't actually care what constraints were forgotten about, just that some constraints were.

\paragraph{File names}
Some error messages mention the file name.
For example, universe names are prefixed with the module path of the toplevel environment, which contains the filename.
Since we create temporary files with random names to test out code changes, we need to filter out such details.

\paragraph{Whitespace}
In some cases, minor changes in any of the above ways might result in different word-wrapping.
Hence we don't want to consider whitespace changes as significant changes in the error message.

\subsubsection{Inlining Dependencies}\label{sec:inline-deps}

\paragraph{The Basics}

Suppose we are developing a project called \verb|Example| and we have a file \verb|Foo.v| with the contents
\begin{verbatim}
Lemma complicated : True = False.
Proof.
  (* ... *)
  (* thousands of lines of proof taking minutes to compile *)
  (* ... *)
Defined.
Lemma irrelevant1 : 1 = 1.
Proof. reflexivity. Qed.
\end{verbatim}
and a second file \verb|bug.v| with the contents
\begin{verbatim}
Require Import Example.Foo.
Lemma irrelevant2 : 2 = 2.
Proof. reflexivity. Qed.
Lemma boom : True -> False.
Proof. rewrite complicated. Qed.
\end{verbatim}

We want a standalone reproducing test-case, and therefore would like to inline dependencies.
After minimizing \verb|bug.v| as much as possible, it will look like
\begin{verbatim}
Require Example.Foo.
Import Example.Foo.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

We would now like to inline \verb|Foo.v|.
If we had not split apart \verb|Require| from \verb|Import|, we could just drop the file in place of the \verb|Require|.

However, we must be able to handle \verb|Import|.

Because \verb|Import| can use qualified names, we wrap the contents in modules, \verb|Export|ing any directory modules because we must be able to refer to \verb|complicated| in \verb|Foo.v| as \verb|Foo.complicated| and \verb|Example.Foo.complicated|:
\begin{verbatim}
Module Export Example. Module Foo.
Lemma complicated : True = False.
Proof.
  (* ... *)
  (* thousands of lines of proof taking minutes to compile *)
  (* ... *)
Defined.
Lemma irrelevant1 : 1 = 1.
Proof. reflexivity. Qed.
End Foo. End Example.
Import Example.Foo.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

This file can then be minimized to
\begin{verbatim}
Lemma complicated : True = False.
Admitted.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

\paragraph{Multiple Modules}

\todo{this section}

\paragraph{Nested Modules vs.\ \texttt{Include}}

\todo{this section}

\paragraph{Adjusting References}

\todo{this section}

\paragraph{Where do the \texttt{Require}s go?}

\todo{this section}

\paragraph{Diamond Problems and Side-Effects of \texttt{Require}}

\todo{this section}

\paragraph{All at Once, or One at a Time?}

\todo{this section}


\begin{comment}

\subsection{Adaptations specific to Coq}
Coq has a number of quirks that enable efficient bug minimization, as well as a few quirks that seriously hinder bug minimization.
The quirks that enable efficient bug minimization are:
\begin{itemize}
\item lack of forward-references (definitions cannot refer to any identifiers that have not yet been added to the global environment);
\item emitting text files that describe how to hyperlink identifiers, for documentation purposes;
\item ending every comamnd in either one or three full stops, with only two exceptions (\verb|{| and \verb|}|);
\item
\end{itemize}
\subsubsection{Adaptations enabled by lack of forward references}
Because Coq forbids forward references, we can remove all lines after the line where the error occurs without risking a change in error messages.

Additionally, we can, by and large, remove lines in reverse order.
That is, if we removing line $n$ changes the error message, then first removing line $m$ for $m < n$ will almost never result in a success (that is, a lack of error message change) from removing line $n$ later.
This allows us to rely on a single work-horse pass that removes lines, one at a time, starting from the end of the file.

We say ``lines'' above, but in fact a line (a string of characters ended by a newline) is not the natural statement
\subsubsection{Comment Stripping}







Specific adaptations to delta debugging for working with Coq scripts, including
admitting proofs
splitting imports and requires
working in units of "an entire proof script / definition"
being able to work from the end of the file, due to lack of cyclic dependencies (with some minor exceptions, such as program obligations)
the work that it takes to inline one file into another
What specific features (both ones that are already present and ones that are requested) are necessary for producing self-contained single-file test-cases
Integration of delta-debugging into CI failures, in particular:
This is feasible compute-wise (+ some graphs / stats on typical minimization runtime)
We found an interaction mode that seems to work well (+ some feedback on other interaction modes)
(2)
I'm not sure what the overall story for the paper should be ("we did a cool thing, here it is"? or "we did a useful thing, here's what it took and what you need to know if you want to do a similar thing"? or something else?)
Here are some sections that are probably worth including:
What the bug minimizer does (Coq project + error message -> minimal test case) and how it works (delta debugging on maintaining the error message + various details)
How the integration with Coq's CI and GH Actions and coqbot works and what it does (allows PR authors to turn "this failed" into an actually useful test-case that can be further minimized and/or included in the test-suite and/or mined for insight into what changed, maybe note also that this improves on debuggability of error messages --- the error message + source is generally not enough to figure out what's going wrong when the CI fails, Coq doesn't have unit tests because ???, this closes the gap)
Future work including:
Automatic minimization and maintenance of test-cases for bugs reported on Coq's issue tracker
Multi-file minimization
Better support for when bumping the branch targeted by the CI causes a failure (rather than it being a change in Coq)
Various issues that prevent complete inlining of files

\end{comment}

\section{Future Work}\label{sec:future-work}
\todo{organize this section}

Rather than emitting the information to stderr when wrapping coqc, we could emit it to a file (have to be careful about paralell builds, though).
See \url{https://github.com/coq-community/run-coq-bug-minimizer/issues/11}.

Use a proper Coq parser.
See \url{https://github.com/JasonGross/coq-tools/issues/56}.

When removing a statement fails, try removing all statements that generate different error messages, seeing if we can recover the original error message.
This should probably be done with incremental compilation.
This would allow us to drop many specific passes (such as removing empty sections and modules).
Furthermore, if we hook into a proper Coq parser that allows discovering well-balanced structure more fine-grained than a particular definition, we could plausibly use this sort of thing to be able to remove specific fields from records, for example.
See \url{https://github.com/JasonGross/coq-tools/issues/73} and \url{https://github.com/JasonGross/coq-tools/issues/88}.

Make use of \verb|Set Suggest Proof Using| to enable removal of proof scripts that only matter for specifying the arguments of a lemma.
See \url{https://github.com/JasonGross/coq-tools/issues/68}.
Example:
\begin{verbatim}
Section foo.
  Context (x y : nat) (p q : x = y).
  Lemma silly : True -> x = y.
  Proof. intro; exact p. Qed.
End foo.
Definition bar := @silly 0 0 eq_refl I.
Fail Check bar.
\end{verbatim}
This file should be able to be minimized to
\begin{verbatim}
Section foo.
  Context (x y : nat) (p q : x = y).
  Lemma silly : True -> x = y.
  Proof using p. Admitted.
End foo.
Definition bar := @silly 0 0 eq_refl I.
Fail Check bar.
\end{verbatim}
This is useful when the proof script is much longer than \verb|intro; exact p|.

Support incremental compilation.
Removing a line at the end of the file should not require recompiling the initial segment.
See \url{https://github.com/JasonGross/coq-tools/issues/87}.

Remove well-balanced structures (sections, modules, etc) before we try to remove lines in those structures.
See \url{https://github.com/JasonGross/coq-tools/issues/89}.

\section{Evaluation of Results}

\todo{Theo}

\subsection{Research questions}

We want to evaluate how well the Coq bug minimizer and its integration into Coq's CI setup, via coqbot, works to achieve our stated objectives which we remind below:

\begin{enumerate}
\item Improve the coverage of the test suite by systematically extracting minimal test cases from compatibility issues that are detected in external projects tested in Coq's CI.
\item Help Coq developers assess more easily the nature of compatibility issues they introduce with their changes.
\end{enumerate}

For this purpose, we specify several research questions to investigate:

\begin{description}
\item[RQ1:] How often does the minimizer successfully produce a reduced example from the CI failures it was triggered on?
\item[RQ2:] What time does it take to run on the cases where it was triggered?
\item[RQ3:] How often does it manage to produce a reduced case that is sufficiently short and does not depend on anything beyond the standard library of Coq?
\item[RQ4:] Does it help Coq developers understand CI failures?
\item[RQ5:] Does it help Coq developers expand the coverage of the test suite?
\end{description}

\subsection{Data collection}

To support replicating the results, all our data collection and analysis code is made available as Jupyter notebooks and our datasets are made available as CSV files.\footnote{\todo{Theo}}

To know in which context the bug minimizer was triggered and how it answered, we fetch comments from pull requests in the GitHub repository using GitHub's GraphQL API.
%
We look for all the pull requests with the words ``coqbot ci minimize''.
%
These correspond to all the pull requests where coqbot offered the possibility of minimizing a CI failure. For each of these pull requests, we fetch all the comments, with their author, time and date and body text.
%
We detect triggers of the minimizer by looking at comments by authors other than coqbot with the words ``coqbot ci minimize''.
%
We exclude pull requests authored by one of the co-authors of this paper as these pull requests were mostly for testing the minimizer integration and debugging issues.

After several months in production, we added a sentence to the comment posted by coqbot with the results of the minimizer asking users to fill a very short survey about their experience with it.
%
However, this message alone did not attract any answer (probably because it was not visible enough, or not targetted enough to specific users).

To alleviate this problem and also to collect data on past runs of the minimizer, we posted an issue in Coq's bug tracker tagging all the people that triggered the bug minimizer or were the author of a pull request where the bug minimizer was triggered.
%
For each of the tagged users, we listed the pull requests that were relevant to them (either as an author or as a triggerer) and provided a link to the survey (with the pull request number pre-filled).
%
The survey asked in particular how useful the minimizer was to the respondent, if it helped understand the impact of the pull request on external projects and if the minimized test case was used to extend the test suite.
%
The posted issue was auto-generated from our data on the bug minimizer runs.

This issue allowed us to collect 19 answers from 7 different respondents. \todo{Update if needed.}
%
The data collected through this survey are also available as a CSV file, which we import and analyze in our Jupyter notebook.

\subsection{Data analysis}

To answer our first three research questions, we look at all the pull requests and all the projects where the CI minimizer was triggered.
%
To avoid double-counting multiple runs on the same CI failure, we only look at the first CI minimizer triggers on a given pull request and a given project.

When the minimizer is triggered, the bot answers with a comment ``I have initiated minimization \ldots'' or ``I am now running minimization \ldots'', providing the list of projects on which it is being run.
%
Next, when it finishes minimizing a project, it produces a comment starting with ``Minimized File''.
%
The comment contains the minimized file, truncated to 32KiB (half of GitHub's comment limit), which generally amounts to about 700 lines.
%
The comment may also contain ``interrupted by timeout, being automatically continued'' if the minimization process timed out and has to be restarted to go further, which the bot automatically does.
%
We ignore these comments, only looking for final reduction outputs.
%
Finally, the bot posts a comment starting with ``Error: Could not minimize file'' when it was not able to minimize the requested failure, for instance, because it could not reproduce it or could not reproduce the successful run on the base branch.

We match comments indicating the start of the minimization with comments indicating the end of it, and use these two comments to determine if the minimizer was able to produce a reduced example, how long it took and if the reduced example was sufficiently short and self-contained.

\subsection{Results}

\subsubsection{RQ1: How often does the minimizer produce a reduced example?}

We have identified 155 CI failures for which minimization was started (very often, several minimization runs are started in the same comment).
%
For 131 of these (85\%), the bot eventually answers with a reduced example.
%
In 16 cases, the bot reports that minimization was unsuccessful.
%
\todo{Investigate manually the 16 cases and report on the causes.}
%
In 8 cases, there was no matching comment marking the end of minimization.
%
\todo{Investigate manually the 8 cases and report on the causes.}

\subsubsection{RQ2: What time does it take to run on the cases where it was triggered?}

On the 131 successful runs, we compute the duration as the time delta between the start and the end comments.
%
Note that while this precisely evaluates the time that developers using the minimizer had to wait for the results, it can overestimate the actual time spent by the minimizer on the example, if the minimizer run was queued for a while before starting to run in GitHub Actions.

The minimal observed duration is a little above 2 minutes (131 seconds) and the maximum observed duration is close to 20 hours (71665 seconds).
%
50\% of the runs take less than 15 minutes (910 seconds).
%
70\% of the runs take less than 53 minutes (3208 seconds).
%
80\% of the runs take less than 108 minutes (6467 seconds).
%
90\% of the runs take less than 5.3 hours (19219 seconds).
%
This is to compare with the time it can take to build a single project (sometimes, more than an hour).

\todo{An analysis project by project, comparing with the time it takes to simply build the project?}

\subsubsection{RQ3: How often does it produce a short and self-contained test case?}

\todo{}

\subsubsection{RQ4: Does it help Coq developers understand CI failures?}

We got 19 answers to our survey, from 7 different people.
%
14 of these answers where about pull requests where the respondent had triggered the minimizer themselves (and 5 where about pull requests which the respondent authored but where they did not trigger the minimizer).

The answers we got are very contrasted between the triggerers and the non-triggerers.
%
On the one hand, for each of the 5 responses from authors who did not trigger the minimizer themselves, the response to the survey indicated that the minimizer was not useful to the respondent and did not help them understand the CI failure.
%
On the other hand, 9 out of 14 responses from triggerers indicated that the minimizer was useful (3 somewhat useful and 6 very useful).
%
The next question on whether it helped diagnose the CI failure was optional but a majority of the triggerers that answered (5 out of 9) said it had helped.

\todo{Write about why people said the minimizer was not useful.}

\subsubsection{RQ5: Does it help Coq developers expand the coverage of the test suite?}

We also asked to the users of the minimizer if they used the output of the minimizer to extend the test suite of Coq (as the minimizer itself suggests when its reduced test case is short enough to fit in a comment).
%
The users who did not find the minimizer useful never answered positively to this question.
%
However, among the users who found the minimizer useful, a majority (5 out of 8) said they had used the output of the minimizer to extend the test suite of Coq.
%
In one case, the respondent explained that while the minimizer had been very useful by quickly identifying the reason of the CI failure, they had not used its output to extend the test suite because the reason was that the tested project used a deprecated feature that was being removed.

\todo{Manual analysis of how people extended the test suite}

\subsection{Threats to validity}



%\nocite{*}%forbidden by lipics

%% Bibliography
\bibliography{coq-bug-minimizer.bib}


%% Appendix
\appendix
\section{Appendix}

Text of appendix \ldots

\end{document}
