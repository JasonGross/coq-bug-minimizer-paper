\documentclass[a4paper,USenglish,cleveref,autoref,thm-restate]{lipics-v2021}
%This is a template for producing LIPIcs articles.
%See lipics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Automatic Bug Minimization in Proof Assistants: A Case Study in Coq}
\titlerunning{Automatic Bug Minimization in Coq}%TODO optional, please use if title is longer than one line

\author{Jason Gross}{CSAIL, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA \and MIRI, USA \and \url{https://jasongross.github.io/} }{jgross@mit.edu}{https://orcid.org/0000-0002-9427-4891}{}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional. Use additional curly braces to indicate the correct name splitting when the last name consists of multiple name parts.

\author{Théo Zimmermann}{Inria, Université de Paris, CNRS, IRIF, F-75013, Paris, France \and \url{https://www.theozimmermann.net} }{theo@irif.fr}{https://orcid.org/0000-0002-3580-8806}{}

\author{Miraya Poddar-Agrawal}{Reed College, 3203 SE Woodstock Blvd, Portland, OR 97202, USA% \and \url{https://github.com/miraya-pa}
}{ragrawal@reed.edu}{https://orcid.org/0000-0001-7617-9180}{}

\author{Adam Chlipala}{CSAIL, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA \and \url{http://adam.chlipala.net/} }{adamc@csail.mit.edu}{https://orcid.org/0000-0001-7085-9417}{}

\authorrunning{J. Gross and T. Zimmermann and M. Poddar-Agrawal and A. Chlipala} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Jason Gross and Théo Zimmermann and Miraya Poddar-Agrawal and Adam Chlipala} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011111.10011113</concept_id>
       <concept_desc>Software and its engineering~Software evolution</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011074.10011111.10011696</concept_id>
       <concept_desc>Software and its engineering~Maintaining software</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011041</concept_id>
       <concept_desc>Software and its engineering~Compilers</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011074.10011099.10011692</concept_id>
       <concept_desc>Software and its engineering~Formal software verification</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Software and its engineering~Software evolution}
\ccsdesc[500]{Software and its engineering~Maintaining software}
\ccsdesc[100]{Software and its engineering~Compilers}
\ccsdesc[300]{Software and its engineering~Formal software verification}
%\ccsdesc[100]{\textcolor{red}{Replace ccsdesc macro with valid one}} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm

\keywords{debugging, bug minimization, Coq, automatic test-case minimization}

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\acknowledgements{I want to thank \dots\todo{update or remove}}%optional

%\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{usebib}
\newbibfield{title}
\bibinput{coq-bug-minimizer}
\newcommand{\citetitle}[1]{\emph{\usebibentry{#1}{title}}\nocite{#1}}

\makeatletter
\newcommand{\todo}[1]{%
\@latex@warning{TODO: \detokenize{#1} on page \thepage}%
\textcolor{red}{[\textbf{TODO:} #1]}}%
\makeatother

\begin{document}

\maketitle

\begin{abstract}
  We present the Coq Bug Minimizer, a tool for \emph{reproducing buggy behavior} in \emph{minimal} and \emph{standalone} files.
  We have integrated this tool with coqbot to trigger \emph{automatically} on reverse CI failures.
  Our tool eliminates the overhead of having to download, set up, compile, and then explore and understand large developments.

  In this paper, we describe the insights that we expect to generalize to bug minimization in other proof assistants.
  We evaluate the Coq Bug Minimizer on over 150 CI failures.
  Our tool succeeds in reducing the failure to a smaller test case in roughly 70\% of these cases.
  50\% of successful runs take less than 20 minutes, and 80\% complete within about two hours, which is comparable to the length of time spent running the CI on a single pull request.
  The minimizer produces a fully standalone example 97\% of the time, and the example is on average about one-third the size of the original test.
  The average reduced test-case compiles in 1.25 seconds, with 75\% taking under half a second.
\end{abstract}

\section{Introduction}

In the world of machine verification, the dream is to prove the correctness of every program.
Projects such as Coq Coq Correct~\cite{coq-coq-correct} make significant progress towards this dream for even our most foundational tools: proof assistants themselves.
However, large swathes of proof-assistant software—such as tactic languages, elaboration hints, and document managers—remain unproven, oftentimes even unspecified, and without adequate test-suite coverage.


To reduce the impact of accidental changes in parts of the proof assistant Coq not covered by the test suite, starting in 2017, developers adopted ``reverse'' continuous integration (CI)~\cite{zimmermann:tel-02451322} (also known as Reverse Dependency Compatibility Testing~\cite{ochoa2022breakbot}).
Every change in Coq is now tested against a suite of external Coq projects maintained in different repositories by different teams.
Every time a change breaking an external project is deliberately introduced, developers of Coq will fix the compatibility issues in external projects to prevent this crowdsourced test suite from shrinking for future releases.


The external projects are large and intricate, so debugging and fixing failures in the reverse CI is a time- and effort-intensive process.
In order to streamline this process, we have built the Coq Bug Minimizer which \textbf{reproduces buggy behavior} in \textbf{minimal} and \textbf{standalone} files and is integrated with coqbot to trigger \textbf{automatically} on reverse-CI failures.
The Coq Bug Minimizer eliminates the overhead of having to download, set up, compile, and then explore and understand large external projects.
Typically, using the Coq Bug Minimizer reduces the total number of lines of code involved in the buggy behavior by about a factor of three.


The state of the art in bug minimization is delta debugging.
The delta tool~\cite{delta} preprocesses files into well-balanced blocks of code to determine the granularity of code removal, and then it runs binary search on removing blocks of code.
However, bug minimization is different for proof assistants than for compilers of other languages, with unique challenges and applicable tricks.

Our goal with this paper is to make the ITP community aware of the applicability of this idea from the world of software engineering.
We also share what we have learned about where the problem is harder or easier than the traditional one, e.g. correctness of compilers.
Drawing on empirical results from nearly a year of use in Coq's production CI system, we reflect on how effective our style of test-case minimization has been and where the biggest opportunities for improvement remain.
Already, Coq developers are happy with the time the bug minimizer saves them, and our methods may be of interest to the developers of other proof assistants.

The next section introduces the flow of Coq test-case minimization by example, calling out the main elements that go into a working solution.
\autoref{sec:easier} explains what about the minimization problem is easier for Coq sources than traditional programs.
Balancing out those simplifications, we still encountered plenty of twists, which \autoref{sec:harder} summarizes.
\autoref{sec:evaluation} presents our deployment in Coq's production CI, with analysis of how effectively different test cases were minimized; \autoref{sec:related-work} describes connections to related work; and \autoref{sec:future-work} closes with our thoughts on the most worthwhile improvements to make to our tooling.



\section{Test-Case Minimization by Example}\label{sec:example}

Imagine you have just added a new feature to Coq, but as a result a popular Coq-based open-source library stops compiling.
You did not need to identify that library, because the Coq project's continuous integration automatically recompiles many open-source libraries.
It can even report which source file failed to build and with which error message.
However, it is infeasible to be an expert on all Coq libraries out there, and making sense of an error message within a large project can be daunting.
Instead, you hope for the test-case minimizer to construct a single short file that you can analyze independently of the library that inspired it.

As an example that is decidedly \emph{not} enormous but rather easier for us to talk through here, consider this Coq source file.
\begin{verbatim}
FILL IN
\end{verbatim}

Assume the last line shown here has triggered a new bug in Coq.
The minimizer explores the space of file modifications that might simplify human understanding, without suppressing the bug.
The most obvious move is to find lines that can be deleted.
Deleting several candidates here takes us to a new file.
\begin{verbatim}
FILL IN
\end{verbatim}

We still depend on another imported module of this obscure library.
Another useful move is to inline it, like so.
\begin{verbatim}
FILL IN
\end{verbatim}

Now we may find still-further lines that can be deleted, producing this result.
\begin{verbatim}
FILL IN
\end{verbatim}

The whole process may feel fairly intuitive from a description at this level, but let us draw attention to several core tasks.

Finding \emph{which file triggers the bug, with which compilation settings, including path information to find dependency modules} is not trivial.
The process should work with the wide variety of build systems constructed for different Coq libraries.

Deciding \emph{when two Coq runs have hit the same bug} is also more complicated than it may sound.
On the one hand, we want to look for specifity in the details of error messages.
On the other hand, many reasonable file simplifications lead to incidental changes in error messages.

Finally, there is the most obvious challenge of \emph{exploring the space of program simplifications in a smart way}.
Many research papers in the software-engineering community have been written on just this topic, and proof-assistant sources involve some complications that are uncommon in conventional programming, perhaps triggering new challenges.
For instance, highly automated Coq developments often have high compile times even just for single files, so we may need to be more frugal in how many program variants we test.

Each of these categories forced us to make several nonobvious design decisions.
Before proceeding to share the most important ones, we note some ways in which test-case minimization is actually \emph{simpler} in this setting than others.


\section{Simplifications of the Proof-Assistant Setting}\label{sec:easier}

Classic delta debugging~\cite{Zeller2002} uses binary search through program structure to find subprograms that can be removed while preserving relevant properties, like triggering specific bugs in particular compilers.
We found that a simpler method works well for minimizing Coq sources: remove \emph{everything after} the error-message-generating line, and \emph{try removing the syntactic units beforehand in-order, one-at-a-time}.
Reducing the number of ``experiments'' that need to be run on program variants can really pay off when each variant requires significant processing time, but why do we expect this simplified search process to yield good results?

\subsection{Dropping Code After a Bug-Triggering Line}

The first reason is straightforward but still very helpful: Coq source files may not contain forward references.
There is no convention as found in languages from Java to Haskell, where all functions in a file are considered mutually recursive.
As a result, there really should be no way for one error-message-generating line of a file to change behavior based on modifications to later lines.
Actually, we should think in terms of single commands processed by Coq (e.g., function definitions or individual tactics), not lines, but the principles is sound, with that fix-up.

As a result, the satisfyingly simple first step of test-case minimization is removing all lines following the command that triggers the bug.

\subsection{Attempted Dropping of Candidate Lines in Order}

Next, where classic delta debugging does binary search to find segments of source files that can be deleted without masking bugs, we step through all commands in-order, backward, starting after the one that triggers the bug.
Why will we not miss important shrinking opportunities?
One answer is that, even if we do, the savings in running time may be enough to justify the choice.
Our empirical evaluation (\autoref{sec:evaluation}) demonstrates that we generally still realize good results.

However, somewhat speculatively, we believe that it helps to be in a language where \emph{proof irrelevance} is such an important concept.
Most details of a Coq development are supposed to be strongly encapsulated, in the sense that a change in one part should not be observable in another part.
Theorem statements mediate between segments of a development.
While a compiler might break during compilation of one function because of changes to the body of another function, it generally surprises us if one Coq proof breaks because of changes to another proof.
Thus, it is generally safe to mark any proof as skipped (``admitted,'' in Coq parlance), without masking Coq bugs that arise later.
Our minimizer tries its best to drop as many unneeded lemmas as possible, but then the second-best change is to mark admitted as many of their proofs as possible.


\section{Challenges Minimizing Test Cases for Proof Assistants}\label{sec:harder}

There were still plenty of details that motivated nonobvious design decisions in our minimizer.
Let us summarize them, grouped in terms of the three main steps sketched in \autoref{sec:example}.

\subsection{Finding Which Sources to Analyze and with Which Settings}

Before we can begin analyzing a specific source file, we need to take a few steps.
\begin{enumerate}
\item Unpack and install both the succeeding and failing versions of Coq and corresponding developments.
\item Replace the Coq binaries with wrappers that print out the arguments that Coq was called with, as well as \texttt{COQPATH} (an environment variable listing directories to be searched for imported modules) and the current directory.
\item Run Coq on the succeeding and failing developments, ensuring that the version that should pass does in fact pass, and the version that should fail has a recognizable error message.
\item Parse the build log to determine the buggy file name and the arguments to pass to Coq, using the extra logging introduced by our wrappers.
  This workflow means that we need not interface directly with varied build systems of different contributions on the CI.
\item Run Coq on the buggy file.
\item Parse the error message, ensuring that it matches with the error message from the build log.
  (See \autoref{sec:error-messages} for subtleties in that comparison)
\end{enumerate}

Again, the goal of the minimizer is to take a CI development that succeeds on the tip of the master branch and fails on a given pull request (PR), emitting a small, standalone file that succeeds on master and fails in the same way on the PR.
In order to do so efficiently, we reuse the CI artifacts from Coq.
We download the prebuilt versions of Coq from master and from the tip of the PR.
From just these artifacts and the name of the failing CI development, we must assemble enough information to run the bug minimizer.
We replicate Coq's generic CI workflow to install Coq as well as any dependencies of this CI development, into different directories: one for the version of Coq expected to pass and another for the version of Coq expected to fail.
We also reuse Coq's generic CI workflow to figure out the error message and the failing file we want to minimize.

Let us justify the extra information that our Coq wrapper programs log.
We need \texttt{COQPATH} to ensure that we have the right search path for the dependencies of \texttt{coqc}, the command-line Coq compiler.
We need the command-line arguments so that we know what flags to tell the bug minimizer to pass to \texttt{coqc}.
Note that we \emph{must not} change relative paths to absolute ones when passing arguments along to \texttt{coqc}, because the output of \texttt{coqc} is sensitive to the difference between relative and absolute paths, so changes can muddle tests that are meant to produce output files (and did in the past, for example with \texttt{ci-elpi}).
We can locate the error message by looking for the last instance of \texttt{File "$f$", line $\ell$, characters $n$-$m$:} followed immediately by a line beginning with \texttt{Error}.
(Note that warning messages also emit the \texttt{File $\ldots$} line, but we do not want to catch warnings.)
We look for the last instance of the wrapper debug printout information that points at the same file, though, so long as we were careful always to build single-threadedly, we could instead just look for the most recent debug printout before the error message.

Given this information, we adjust the arguments so that we can tell the bug minimizer where the dependencies live both for the passing and failing versions of Coq.
We then pass this information to the bug minimizer:
\begin{itemize}
\item the location of the file to be minimized;
\item the log file containing the error message, which must match the error message that the minimizer believes the file produces;
\item the locations of the \verb|coqc|, \verb|coqtop|, and \verb|coq_makefile| programs for the tip of the PR;
\item the location of the \verb|coqc| program for the master branch;
\item the locations of the dependencies for both the passing and failing versions of Coq, parsed from the command-line arguments and from walking the directories in \texttt{COQPATH};
\item any arguments to \verb|coqc| that are neither naming dependency locations nor known to be both irrelevant to the processing of the file and counterproductive to the minimizer's operation (such arguments are \verb|-batch|, which applies only to \verb|coqtop|; \verb|-time|, which will only make logs of the minimizer much longer; and \verb|-noglob|, \verb|-dump-glob|, and \verb|-o|, which interfere with the generation of outputs used by the minimizer).
\end{itemize}

Henceforce, we will assume that we have all of this information at hand, writing only of how to minimize files.

\subsection{Reproducing Buggy Behavior}\label{sec:error-messages}

How do we know modifications to source files are genuine simplifications that have not masked bugs?
What does it mean to reproduce the ``same'' bug?
We generate a file that succeeds on the previous version of Coq and continues to fail on the modified version of Coq, with the same error message that showed up in the reverse CI.
However, the error message of the generated file does not need to be \emph{exactly} the same as in the original file, so long as the reason for the error message is the same. 
Thus, we modify our goal to reproducing buggy behavior in place of reproducing the ``same'' bug. 

We apply the following relaxations in comparing error messages.
\begin{enumerate}

\item Universe inconsistencies are how Coq prevents users from proving absurdity by assuming a ``set of all sets.''
The explanations of universe inconsistencies in error messages are sensitive to how many universes are floating around and in what order constraints were added.
Rather than requiring output files to mimic the error messages exactly, we only require that they result in \emph{some} universe inconsistency.

\item Any two error messages about ``forgotten universes'' are considered matching, since these tend to arise only from very specific Coq internal errors.

\item Usually differences in numbering, e.g. in universes or autogenerated identifiers, are incidental and are not treated as implying different error messages.  One special case is lengths of universe instances, so we look for the text ``Universe instance should have length'' in the error message and only use number-insensitive comparison if this text is not found.

\item We consider any error messages containing ``Unsatisfied constraints: \ldots\ (maybe a bugged tactic)'' as equivalent, since related bugs are localized to one relatively small part of the Coq implementation, and small changes to a source file can modify constraints significantly.

\item We also ignore filenames and word wrapping in comparing error messages.

\end{enumerate}

\subsection{Minimization}

Why is it useful to minimize test cases?
When the external project exhibiting buggy behavior takes minutes or hours to compile, the edit-compile-test-debug loop is very long, and exploring hypotheses about what went wrong takes a long time.
Test cases that compile in seconds or fractions of a second allow developers to more fluidly work on fixing the bug.
Additionally, shorter test cases can make clearer what is going wrong and help developers formulate hypotheses about their changes to Coq.

Thus, the problems in minimization are making the test as small as possible and quick to compile. 
Another important consideration is getting to our minimal test case as quickly as possible.

We now present our main ideas toward those ends, which are applied within a loop of repeatedly shrinking a file.
The inner loop does not further simplify code newly introduced or modified by itself, but instead we rely on an outer loop that reruns the inner loop so long as progress is being made.

\subsection{Making the Minimization Process Itself Fast}

In our goal to get the shortest reproducing test case as quickly as possible, it helps to first make any changes that might significantly speed up the execution time, and only after we're done with all of the changes that might improve running time should we try to further minimize the file with changes that are unlikely to impact compile time.

The slowest part of almost all Coq developments is proof scripts.
Hence we attempt to remove proof scripts as early as possible.
Since proof assistants check that proofs are valid, we cannot simply remove a proof, like we might remove a function body in a traditional programming language.
However, most proof assistants have some mechanism for ``giving up'' on a proof or ``trusting'' the user, and Coq is no exception.
Its mechanism involves any of \verb|Admitted|, \verb|Admit Obligations|, or the \verb|admit| tactic.
Replacing proof blocks with these commands, rather than just removing proof scripts, allows us to make much smaller and faster examples than might otherwise be possible.

\subsubsection{Finding Textually Smaller Test Cases}

The simplest function of the bug minimizer is to remove unneeded lines.
As noted in the prior section, we try removing one syntactic unit at a time, moving backwards from the unit that triggered the error message.

However, we can easily enough get stuck in local minima, when we remove single commands and check that bug behavior is unchanged.
For instance, there may be an irrelevant lemma that we want to remove.
\begin{verbatim}
Lemma irrelevant : two = 2.
Proof. reflexivity. Qed.
\end{verbatim}
Since Coq forbids nested lemmas, removing statements one-at-a-time will not work, as the state
\begin{verbatim}
Lemma irrelevant : two = 2.
Proof. reflexivity.
\end{verbatim}
results in an error about nested proofs, if there is a theorem afterward.

We instead group statements into \emph{definition} blocks to be removed all at once.
We get information about definitions by parsing the output of \texttt{coqtop -emacs -time}.
This way, we can remove the lemma block all at once.

We could in theory deal with more complicated nesting structure, for example trying to remove an entire section or module at a time.
The delta tool~\cite{delta} is in fact built around preprocessing the file into one that exposes nested structure clearly, then removing well-parenthesized blocks.
However, removing statements, grouped into definitions as necessary, sufficies for removing time-consuming code.

\paragraph{The Program Construct}
One Coq construct that does not fit neatly into this approach is \verb|Program|, where a function definition is associated with following proofs of obligations related to dependent typing.
We cannot just look for \verb|Program| statements followed by \verb|Obligation| blocks to remove all together, because \verb|Obligation| blocks can be interleaved with other definitions.
Luckily, we can replace any obligation block with a use of the \verb|Admit Obligations| command, which admits all remaining obligations -- and it happily handles any case with \emph{no} remaining obligations, so we need not worry about introducing duplicate invocations.

\paragraph{Empty Sections and Modules}
Removing statements one-at-a-time will not always be able to remove empty sections (nor empty \verb|Module|s or \verb|Module Type|s).
That is why we have a pass dedicated to removing empty sections, modules, and module types.

\paragraph{Exporting Modules}
Coq's features to import and export modules (e.g., including all definitions of one module inside another) can create some particularly thorny situations for statement-at-a-time shrinking.
If we remove just an \verb|Import| commands, then later commands fail because important identifiers are out-of-scope.
If we remove just the definition of the imported module, then the \verb|Import| fails.
The solution is to merge these two commands, so that they become a candidate for removal together.
We change \verb|Module| commands into \verb|Module Export| commands to this end.
Often that change renders later \verb|Import| commands redundant, so they are removed by later passes.

\paragraph{Splitting Definitions}
One pass in the minimizer tries to replace traditional definitions with uses of the interactive proof mode, which is a first step toward admitting those proof bodies (i.e., postulating existence of identifiers rather than giving their definitions) in later steps.

\paragraph{Early Removal of Unused Constants}
There are some likely-to-succeed steps that we try early on, which are superseded by removing each and every structured block one at a time but may result in faster minimization.
The primary example of this sort of step is removing tactics, \verb|Variable| and \verb|Context| statements, and definitions which are not referred to at all after their definitions.

\paragraph{Splitting \texttt{Import} and \texttt{Export}s}
It may be the case that users import modules that they never use, such as in \texttt{Import unused1 used unused2}.
To allow eventual removal of \verb|unused1| and \verb|unused2| even when the \verb|Import used| statement cannot be removed, we have a pass that attempts to split such statements into separate \verb|Import| statements, resulting in \texttt{Import unused1. Import used. Import unused2.}

\subsection{Finding Test Cases That Coq Processes More Quickly}

We mentioned how admitting proofs is a very handy step to shrink files and get them processed more quickly.
There are, however, a few gotchas to keep in mind.

The first quirk is around transparency vs. opacity of lemma definitions; that is, whether the generated proof term is accessible to later definitions.
Either choice (transparent vs. opaque) can break some developments.
Marking a proof-mode definition \emph{opaque} will break later definitions that unfold the definition and then perform further tactic-based surgery on it, while marking a proof-mode definition \emph{transparent} could cause previously failing unfoldings to succeed.
Therefore, we always try both styles of marking a lemma admitted.

Some lemma proofs are declared as transparent rather than opaque, where later steps really do depend on their details.
If those dependencies are too specific, then our shrinking heuristics are not going to work well.
However, one common-enough case is where a later definition uses tactics to \emph{unfold} an earlier definition, going on to use other tactics that may very well be able to adapt to changes in that definition.
There are at least two different ways to mark a proof as admitted, which switch up whether the associated definition is considered transparent or opaque.

Additionally, we may want to admit some parts of the proof script without replacing all of it.
Currently, we use a rather conservative heuristic:
Coq has a tactical \verb|abstract| that executes the tactic it is passed as an argument, making the resulting proof term opaque.
Such subproofs should be able to be replaced with \verb|admit| without changing the behavior of the proof script.
The details are a little subtle, e.g. to avoid changing which section variables a proof depends on and thus changing its type outside the section.

\label{text:ltac-fragment:orig}

\subsection{Finding Test Cases with Minimal Outside Dependencies}

While Coq bugs may be exposed thanks to the complex structure of external libraries, it is difficult for Coq developers to understand all of those libraries well enough to diagnose root causes.
For that reason, we prefer to create minimized test cases that are as standalone as possible, not importing modules from other source files, beyond the Coq standard library.

All of the steps above are applied in a single pass of a process that loops until no further progress is possible.
After we have tried all possible command deletions and kept the ones that preserved the bug, we consider finding a module dependency to inline.
Inlining dependencies is an interesting technical challenge, due to nuances in how Coq binds names and handles global state.

\subsubsection{Naive Strategy}\label{sec:inline-deps}

Suppose we are developing a project called \verb|Example| and we have a file \verb|Foo.v| with the contents
\begin{verbatim}
Lemma complicated : True = False.
Proof.
  (* ... *)
  (* thousands of lines of proof taking minutes to compile *)
  (* ... *)
Defined.
Lemma irrelevant1 : 1 = 1.
Proof. reflexivity. Qed.
\end{verbatim}
and a second file \verb|bug.v| with the contents
\begin{verbatim}
Require Import Example.Foo.
Lemma irrelevant2 : 2 = 2.
Proof. reflexivity. Qed.
Lemma boom : True -> False.
Proof. rewrite complicated. Qed.
\end{verbatim}

We want a standalone reproducing test case and therefore would like to inline dependencies.
After minimizing \verb|bug.v| as much as possible, it will look like
\begin{verbatim}
Require Example.Foo.
Import Example.Foo.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

We would now like to inline \verb|Foo.v|.
If we had not split apart \verb|Require| from \verb|Import|, we could just drop the file in place of the \verb|Require|.
However, we must be able to handle \verb|Import|.

Because \verb|Import| can use qualified names, we wrap the contents in modules, \verb|Export|ing any directory modules because we must be able to refer to \verb|complicated| in \verb|Foo.v| as \verb|Foo.complicated| and \verb|Example.Foo.complicated|:
\begin{verbatim}
Module Export Example. Module Foo.
Lemma complicated : True = False.
Proof.
  (* ... *)
  (* thousands of lines of proof taking minutes to compile *)
  (* ... *)
Defined.
Lemma irrelevant1 : 1 = 1.
Proof. reflexivity. Qed.
End Foo. End Example.
Import Example.Foo.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

This file can then be minimized to
\begin{verbatim}
Lemma complicated : True = False.
Admitted.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

\subsubsection{Shadowing and Name Resolution}

When we want to inline \emph{multiple files in the same project}, the na\"ive strategy would result in multiple modules of the same name.
Coq does not allow multiple modules with the same qualified name, so to avoid this issue, we wrap all modules in uniquely named placeholder toplevel modules.

When we inline a file, \emph{the flags that were used to compile that file} might be different from the flags used to compile the buggy file we are minimizing.
While we cannot do anything about flags that change the behavior of code, such as \texttt{-set 'Universe Polymorphism'}, the most common flags that differ will be flags defining the search path and module binding names.
In this case, we can take advantage of the fact that Coq installs \texttt{.glob} files which describe the absolutization of every \texttt{Require}, \texttt{Import}, and \texttt{Export} in the file.
We use this information when inlining the file to account for such differences.
Additionally, there are some cases where this information allows us to adjust qualified references to the module currently being defined; in this case, we try replacing such references with the mangled name used in wrapping.

\subsubsection{Non-linearizability}

It can be surprisingly difficult to break some import-related statements into sequences of more fundamental statements, which may be considered independently for removal.

\paragraph{Splitting \texttt{Require}s}
Much like \verb|Import| and \verb|Export| statements, we also want to split \verb|Require| statements, and furthermore want to split \verb|Require Import foo| into \texttt{Require foo. Import foo.}
This last splitting will allow us to inline \verb|foo|, as described in \autoref{sec:inline-deps}.
However, there are three subtleties associated with such statements.

First, it may be the case that our bug depends on \verb|Require foo| only because the file \verb|foo.v| contains \verb|Require bar|, and in fact the bug only needs access to the constants defined in \verb|bar.v|.
Hence we have a pass that performs the transitive closure of the dependency relation, inserting \verb|Require| statements at the top of the file for all transitive dependencies of the file being minimized.
Note that if we were more careful, we would not insert all dependencies at the top of the file but instead at each \verb|Require| statement, because \verb|Require| has side effects that may cause the file to fail to compile (see further discussion shortly).
Because we insert the \verb|Require|s in dependency order, removing one statement at a time in reverse order will give us the minimal \verb|Require|s needed to reproduce the error message.

Second, it may be the case that \verb|Require Import foo bar| is not the same as \texttt{Require foo. Require bar. Import foo. Import bar.}.
It may be the case that \verb|Import foo| results in a different module \verb|bar| becoming available.
Alternatively, it may be the case that \verb|Require bar| transitively requires another \verb|foo| file which is different than the one that is resolved by default as \verb|Require foo|, and then \verb|Import foo| will choose the most recently \verb|Require|d module \verb|foo|.

The solution to this problem relies on the fact that \verb|coqc| emits \emph{globalization} files, which contain information about where bits of source code point.
These \verb|.glob| files are primarily used to create hyperlinked HTML versions of \verb|.v| files, but we can co-opt them to get the information we need.
By generating a \verb|.glob| file for our buggy file and reading it, we can determine the absolute paths of all \verb|Require|, \verb|Import|, and \verb|Export| statements, allowing us to split them as needed without worrying about changes to name resolution.

Third and finally, Coq supports statements such as \verb|From Coq Require Import Arith|.
Na\"ively replacing this with \verb|From Coq Require Import Coq.Arith.Arith| results in an error.
Hence, after absolutizing module paths for a given statement, we carefully remove the \texttt{From \ldots} portion of the statement.


\paragraph{Diamond Problems and Side Effects of \texttt{Require}}

The \texttt{Require} command results in many side effects, including global setting of flags, opacity, and argument status; behavior of \texttt{auto with *}; hint databases; global overwriting of Ltac definitions; presence or absence of constants that change the behavior of built-in tactics such as \texttt{tauto}; and even the presence of constants with certain kernel names can change shadowing behavior.
As a result, there is no bullet-proof method of merging files.
However, we can ameliorate the issue by trying multiple strategies of merging files.
We try inserting the file being inlined at the top of the file, as well as at the location where it is \texttt{Require}d.
In the future, we might also want to try moving \texttt{Require}s up higher in the file, to try to handle more situations.

\paragraph{All at Once, or One at a Time?}

We have a flag that allows inlining dependencies all at once.
While this was originally the way all files were minimized, having to process such a large file slowed down minimization drastically, often resulting in minimization times of multiple weeks.
As a result, the current default behavior is to minimize the current file before inlining other files.

\subsection{Workarounds for Persistent Bugs in Coq}

Sometimes even the version of Coq before a change has its own bugs to work around!
One common issue, sufficiently unlikely to be fixed by the Coq developers, is sensitivity to how many modules some code is wrapped inside.
\todo{cite Coq issue}
To work around this problem, instead of wrapping code in nested modules, we can wrap it in a single mangled module and then \texttt{Include} that module into one of the correct qualified name.
However, this strategy will sometimes result in failure of code that refers to the currently-being-defined module to qualify names.
Therefore, we take the overall approach of trying all the different ways of inlining dependencies and using whichever one works, since no single strategy is currently a silver bullet.


% Maybe this part should return later.
%% Where do the \texttt{Require}s go?

%% Coq warns that \texttt{Require}s inside modules are deprecated.
%% While we haven't encountered any issues from wrapping additional modules around \texttt{Require}s, we do try strategies of moving the \texttt{Require}s up to the top of the file.
%% \todo{exmaple?}


\section{An Alternative Usage Mode}

Up to this point, we have talked about using the Coq Bug Minimizer exclusively to minimize reverse-CI failures for debugging faulty changes in Coq.
Our tool can also be used to minimize test cases for newly found bugs in Coq.
In this mode, a bug reporter can write a shell script that invokes a \emph{single} version of Coq to produce buggy behavior on some Coq file, asking coqbot to produce a minimal example from this script.
When running in this mode, we place an additional constraint on the minimizer that the proof script generating the error message should be left untouched,
which allows bug reporters to write proof scripts such as
\begin{verbatim}
some_tactic;
let G := match goal with |- ?G => G end in
lazymatch G with
| buggy_goal => fail 0 "bug remains"
| _ => fail 0 "bug disappeared!" G
end.
\end{verbatim}
to customize the desired reproducing case, trusting that the entire file will not be minimized to something silly like \verb|Goal False. fail 0 "bug remains"|.





\section{Evaluation of Results}\label{sec:evaluation}

\subsection{Research questions}

To evaluate the usefulness of our bug minimizer, we specify several research questions to investigate:

\begin{description}
\item[RQ1:] How often does the minimizer successfully produce a reduced test case from the CI failures it was triggered on?
\item[RQ2:] How often is this reduced test case fully standalone (no dependencies other than Coq's standard library)?
\item[RQ3:] How long does it take to produce such reduced test cases?
\item[RQ4:] What is the size of the reduced cases?
\item[RQ5:] How long do the reduced cases take to run?
\item[RQ6:] What is the amount of code reduction?
%% \item[RQ:] Does it help Coq developers understand CI failures?
%% \item[RQ:] Does it help Coq developers expand the coverage of the test suite?
\end{description}

\subsection{Data collection and analysis}

To support replicating the results, all our data collection and analysis code is made available as Jupyter notebooks and our datasets are made available as CSV files.\footnote{\todo{Theo}}

We retrieve the runs of the bug minimizer by looking for pull requests in the Coq GitHub repository with the words ``coqbot ci minimize'', and we fetch all comments from the bot (timestamp and body text) from these pull requests using GitHub's GraphQL API.
We exclude pull requests opened by the first author as most of these pull requests were for testing the minimizer integration and debugging issues.
When the minimizer is triggered, the bot answers with a comment ``I have initiated minimization \ldots'' or ``I am now running minimization \ldots'', providing the list of projects on which it is being run.
%
Then, when it finishes minimizing a project, it produces a comment with the minimized file.
%
This file starts with header comments containing useful information about the minimization process. %truncated to 32KiB (half of GitHub's comment limit), which generally amounts to about 700 lines.
%
The comment may also contain ``interrupted by timeout, being automatically continued'' if the minimization process timed out and has to be restarted to go further, which the bot automatically does.
%
We ignore these comments, only looking for final reduction outputs.
%
Finally, the bot posts a comment starting with ``Error: Could not minimize file'' when it was not able to minimize the requested failure, for instance, because it could not reproduce it or could not reproduce the successful run on the base branch.

We match comments indicating the start of the minimization with comments indicating the end of it, and use these two comments to determine if the minimizer was able to produce a reduced test case, how long it took, and to answer our other research questions.
To avoid double-counting multiple runs on the same CI failure, we only look at the first bug minimizer trigger on a given pull request and a given project.

\subsection{Results}

\subsubsection{RQ1: How often does the minimizer produce a reduced test case?}

Looking only at the first minimization runs for a given pull request and project, we have identified 196 runs on 51 pull requests (very often, several minimization runs are started in the same pull request on different projects).
On these 196 runs, 73\% succeeded in producing a reduced test case. We count as failed runs the ones where the bot reported ``Error: Could not minimize file'', the ones where we could not find a comment marking the end of minimization, and the ones where the bot answered with a minimized file but this file was not actually reduced from the initial test case (this is something that we can detect from the header comments).

\todo{Investigate the various cases and report on the causes.}

\subsubsection{RQ2: How often is this reduced test case fully standalone?}

We consider that a reduced test case will be most useful if any dependency beyond Coq's standard library was successfully inlined. This means that it will be possible to run the reduced test case without needing to import any additional dependency and will make it more likely that the test case can be added to Coq's test suite. In practice, when minimization successfully produced a reduced test case, it is virtually always standalone (97\% of the cases).

\todo{Talk about the remaining cases.}

\subsubsection{RQ3: How long does it take to produce such reduced test cases?}

We compute the duration of minimization as the time delta between the start and the end comments.
This is an overapproximation of the actual time spent in the minimization process, since it also includes time setting up a VM, and possibly waiting in the queue for an available runner.
%
%Note that while this precisely evaluates the time that developers using the minimizer had to wait for the results, it can overestimate the actual time spent by the minimizer on the example, if the minimizer run was queued for a while before starting to run in GitHub Actions.
We can look at this duration for both successful and failed runs.

For failed runs, we observe that the average duration for the minimization to conclude is 5 minutes (306 seconds) and that the maximum duration is 15 minutes (890 seconds). This is not surprising as it means that time spent beyond that will always be spent performing actual minimization tasks.

For successful runs, we observe more variety. The minimum duration is 4 minutes (232 seconds), the maximum duration is 20 hours (73072 seconds), and the average is 104 minutes (6238 seconds). 50\% of the successful runs finish in under 20 minutes (1218 seconds), and 80\% finish in under 140 minutes (8396 seconds). This number is still reasonable compared to the time that contributors routinely spend waiting for the results of Coq's CI~\cite{zimmermann:tel-02451322}.

\subsubsection{RQ4: What is the size of the reduced cases?}

For these last three questions, we focus only on successful minimization runs that produced a fully standalone file.

The shorter the reduced test case, the more useful it is: it can help developers understand the problem more quickly, and it makes it more likely that it will be added to the test suite. Here again, there is some variety in the size of the reduced cases (counted in number of lines). The average size is 369 lines and the maximum size is 3804 lines. But 25\% of the reduced cases are under 35 lines, 50\% are under 122 lines, and 75\% are under 461 lines.

Developers have the possibility of performing additional minimization manually and restart the automatic minimization process on their manually reduced case. This can help obtain even more reduced cases, but we have not evaluated this feature quantitatively.

\subsubsection{RQ5: How long do the reduced cases take to run?}

A recent feature of the minimizer is that it reports on the expected \texttt{coqc} compile time as part of the header comments it includes in the minimized file. Out of the 43 cases where we had this data available, we obtain that the reduced test cases take on average 1.25 seconds to run, although 75\% of them take under half-a-second, while the maximum time is 26.5 seconds.

\subsubsection{RQ6: What is the amount of code reduction?}

To compute how much code reduction there was, we use data that the minimizer records about each minimization step (how many lines it started from and how many lines it ended up with). These numbers go up at times because of the process of inlining external dependencies. On the other hand, dependencies are only inlined if they couldn't be simply removed, so these numbers do not include the size of the files that were previously imported but did not need to be inlined during the minimization process.

We aggregate these numbers by simply taking the sum of the differences in line number at the beginning and the end of each minimization step. We compute the amount of code reduction by taking the ratio of the final size over the total test case size, defined as being the sum of the final size and the total number of removed lines. We obtain an average figure of 33\%, which means that the final test case size is on average one-third the size of the original test (including the dependencies which actually matter for the test case).

If we compute the size difference only looking at the initial file we started from and the final file we obtained, without accounting for the inlined dependencies, then we get an average ratio of 60\%, which means that the final file is on average 40\% smaller than the file we started from. Note that because of the inlining of dependencies, nothing prevents the reduced test case to be sometimes longer than the file we started from, and this does happen in 28 out of 139 cases. If we look only at the 111 cases for which there was some code reduction, we get that the average reduction is by a factor of 4. If we look only at the 28 cases for which there was code expansion, we get that the average expansion is by a factor of 2.

%% \subsubsection{RQ5: Does it help Coq developers understand CI failures?}

%% We got 19 answers to our survey, from 7 different people.
%% %
%% 14 of these answers where about pull requests where the respondent had triggered the minimizer themselves (and 5 where about pull requests which the respondent authored but where they did not trigger the minimizer).

%% The answers we got are very contrasted between the triggerers and the non-triggerers.
%% %
%% On the one hand, for each of the 5 responses from authors who did not trigger the minimizer themselves, the response to the survey indicated that the minimizer was not useful to the respondent and did not help them understand the CI failure.
%% %
%% On the other hand, 9 out of 14 responses from triggerers indicated that the minimizer was useful (3 somewhat useful and 6 very useful).
%% %
%% The next question on whether it helped diagnose the CI failure was optional but a majority of the triggerers that answered (5 out of 9) said it had helped.

%% \todo{Write about why people said the minimizer was not useful.}

%% \subsubsection{RQ5: Does it help Coq developers expand the coverage of the test suite?}

%% We also asked to the users of the minimizer if they used the output of the minimizer to extend the test suite of Coq (as the minimizer itself suggests when its reduced test case is short enough to fit in a comment).
%% %
%% The users who did not find the minimizer useful never answered positively to this question.
%% %
%% However, among the users who found the minimizer useful, a majority (5 out of 8) said they had used the output of the minimizer to extend the test suite of Coq.
%% %
%% In one case, the respondent explained that while the minimizer had been very useful by quickly identifying the reason of the CI failure, they had not used its output to extend the test suite because the reason was that the tested project used a deprecated feature that was being removed.

%% \todo{Manual analysis of how people extended the test suite}

\subsection{Threats to validity}



\nocite{*}%forbidden by lipics
\todo{remove nocite{*}}


\section{Related Work}\label{sec:related-work}

\todo{Fill in!  Probably mostly drawing on software-engineering examples from outside of proof assistants, but also mentioning vaguely related stuff like QuickChick and hammers (i.e., cases of using testing to optimize the proving process)}


\section{Future Work}\label{sec:future-work}

We were pleasantly surprised to find that several ``shortcuts'' in the logistics behind the minimizer led to good results empirically, but some of these may be worth revisiting to improve results even more.
Our Coq-binary wrappers that log extra information could be changed to use separate log files rather than the same stream as other Coq output, though there are associated challenges in handling parallel builds.
In various places, we use workarounds (like \texttt{.glob} files) to avoid integrating a proper Coq parser, but there would be advantages like being able to remove specific fields from record types.
We remove single commands at a time, rather than removing entire well-balanced command blocks, which probably costs us in minimization time.
Inlining of a dependency is tried only at the precise spot where it is required \emph{or} at the very top of the file being minimized, while trying other locations could be worthwhile.

A broader opportunity is finding related groups of commands that need to be removed together, to avoid changing the error message.
For instance, we might want to move a lemma out of a module, to the top level of a file.
Removing the commands that open and close the module might suffice, even if removing either one alone disturbs the error message.
A general-enough version of this process could replace many specific passes (like removing empty sections and modules).

One remaining aggravation is proper handling of lemma proofs within sections, where the details of the lemma proof influence which section variables are kept in the lemma's type.
We would either use proof-script contents to infer \verb|Proof using| clauses or apply the \verb|Set Suggest Proof Using| command.

There are further-out ideas that could speed minimization significantly but might require significant modifications to Coq itself.
Incremental compilation would be helpful, to save us from rerunning long proof scripts every time we change single lines below them.
Minimizing multiple files in parallel, rather than only inlining files, would allow us to take advantage of multicore execution within single minimization jobs.



%% Bibliography
\bibliography{coq-bug-minimizer.bib}


%% Appendix
\appendix
\section{Appendix}

Text of appendix \ldots

\subsection{Code that was moved}

\subsubsection{Code from \autoref{code:program:1:orig}}\label{code:program:1}
\begin{verbatim}
Require Import Coq.Program.Tactics.
Section __.
Context (arg : True).
Lemma complicated (pf : arg = arg) : True = False.
Admitted.
Axiom bad : False.
Definition another_False := False.
Program Definition irrelevant
  : another_False /\ another_False := conj _ _.
Next Obligation. exact bad. Qed.
Next Obligation. exact bad. Qed.
End __.
Lemma boom : True -> False.
Proof. rewrite (complicated I eq_refl).
\end{verbatim}
\subsubsection{Code from \autoref{code:program:2:orig}}\label{code:program:2}
\begin{verbatim}
Require Import Coq.Program.Tactics.
Section __.
Context (arg : True).
Lemma complicated (pf : arg = arg) : True = False.
Admitted.
Definition another_False := False.
Program Definition irrelevant
  : another_False /\ another_False := conj _ _.
Next Obligation. Admitted.
Next Obligation. Admitted.
End __.
Lemma boom : True -> False.
Proof. rewrite (complicated I eq_refl).
\end{verbatim}

\subsubsection{Code from \autoref{code:program:3:orig}}\label{code:program:3}
\begin{verbatim}
Section __.
Context (arg : True).
Lemma complicated (pf : arg = arg) : True = False.
Admitted.
End __.
Lemma boom : True -> False.
Proof. rewrite (complicated I eq_refl).
\end{verbatim}

\subsubsection{Code from \autoref{code:section:1:orig}}\label{code:section:1}
\begin{verbatim}
Section useless.
End useless.
Module useful.
Lemma complicated : True = False.
Admitted.
End useful.
Lemma boom : True -> False.
Proof. rewrite useful.complicated.
\end{verbatim}

\subsubsection{Code from \autoref{code:section:2:orig}}\label{code:section:2}
\begin{verbatim}
Section useless.
End useless.
Module useful.
Lemma complicated : True = False.
Admitted.
Lemma boom : True -> False.
Proof. rewrite useful.complicated.
\end{verbatim}

\subsubsection{Code from \autoref{code:export-modules:1:orig}}\label{code:export-modules:1}
\begin{verbatim}
Module foo.
Lemma complicated : True = False.
Admitted.
End foo.
Import foo.
Lemma boom : True -> False.
Proof. rewrite complicated.
\end{verbatim}

\subsubsection{Code from \autoref{code:export-modules:2:orig}}\label{code:export-modules:2}
\begin{verbatim}
Module A.
  Module A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module A.
    Axiom A : Set.
  End A.
End B.
Module A1. Export A. End A1.
Module B1. Export B. End B1.
Module Import A2. Import A1. Import A.
 Definition A2 := A. End A2.
Module Import B2. Import B1. Import A.
 Definition B2 := A. End B2.
Check eq_refl : A2 = B2.
\end{verbatim}

\subsubsection{Code from \autoref{code:export-modules:3:orig}}\label{code:export-modules:3}
\begin{verbatim}
Module A.
  Module Export A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module Export A.
    Axiom A : Set.
  End A.
End B.
Module Export A1. Export A. End A1.
Module Export B1. Export B. End B1.
Module Import A2. Import A1. Import A.
 Definition A2 := A. End A2.
Module Import B2. Import B1. Import A.
 Definition B2 := A. End B2.
Check eq_refl : A2 = B2.
\end{verbatim}


\subsubsection{Code from \autoref{code:export-modules:4:orig}}\label{code:export-modules:4}
\begin{verbatim}
Module Export A.
  Axiom A : Set.
End A.
Module B.
  Module Export A.
    Axiom A : Set.
  End A.
End B.
Module Export A1. Export A. End A1.
Export B.
Module Import A2. Import A1.
 Definition A2 := A. End A2.
Definition B2 := A.
Check eq_refl : A2 = B2.
\end{verbatim}

\subsubsection{Code from \autoref{code:export-modules:5:orig}}\label{code:export-modules:5}
\begin{verbatim}
Module A.
  Module Import A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module Import A.
    Axiom A : Set.
  End A.
End B.
Module Import A1. Export A. End A1.
Module Import B1. Export B. End B1.
Module Import A2. Import A1.
 Import A. Definition A2 := A. End A2.
Module Import B2. Import B1.
 Import A. Definition B2 := A. End B2.
Check eq_refl : A2 = B2.
\end{verbatim}
\subsubsection{Code from \autoref{code:export-modules:6:orig}}\label{code:export-modules:6}
\begin{verbatim}
Module A.
  Module Import A.
    Axiom A : Set.
  End A.
End A.
Module B.
  Module Import A.
    Axiom A : Set.
  End A.
End B.
Module Import A1. Export A. End A1.
Export B.
Module Import A2. Import A1. Import A.
 Definition A2 := A. End A2.
Import A. Definition B2 := A.
Check eq_refl : A2 = B2.
\end{verbatim}

\subsubsection{Code from \autoref{code:abstract:1:orig}}\label{code:abstract:1}
\begin{verbatim}
Section foo.
  Context (a : nat).
  Definition foo : nat.
  Proof.
    refine (_ + _).
    all: abstract exact a.
  Defined.
End foo.
Goal True.
  pose (foo 5) as x.
  lazymatch (eval unfold x, foo in x) with
  | ?x + ?y => idtac
  | ?v => fail 0 "bad" v
  end.
\end{verbatim}

\subsubsection{Code from \autoref{code:ltac-support:1:orig}}\label{code:ltac-support:1}
\begin{verbatim}
Module Export AdmitTactic.
Module Import LocalFalse.
Inductive False : Prop := .
End LocalFalse.
Axiom proof_admitted : False.
Ltac admit := abstract case proof_admitted.
End AdmitTactic.
\end{verbatim}
\subsubsection{Code from \autoref{code:ltac-support:2:orig}}\label{code:ltac-support:2}
\begin{verbatim}
Module Export AdmitTactic.
Module Import LocalFalse.
Inductive False : Prop := .
End LocalFalse.
Axiom proof_admitted : False.
Tactic Notation "admit"
  := abstract case proof_admitted.
End AdmitTactic.
\end{verbatim}


\subsection{Text from \autoref{text:ltac-fragment}}\label{text:ltac-fragment}
There are three more corner-cases which result in us using slightly different code in practice.

First, because libraries like HoTT don't load the standard library by default, we need to declare our own \verb|False| inductive and we must also load support for Ltac.
In all versions of Coq prior to 8.12, \verb|Require Import Coq.Init.Notations| was adequate to get Ltac.
In Coq 8.12 and later, we need to either \verb|Require Import Coq.Init.Ltac| (which did not exist in prior versions of Coq) or \verb|Declare ML Module "ltac_plugin"|.
We choose the latter of these two, for compatibility with developments like HoTT which may not have a \verb|Coq.Init.Ltac| file available.

Note that we need to issue \verb|Inductive False : Prop := .| rather than \verb|Inductive False := .|, because any developments that pass \verb|-set "Universe Polymorphism"| on the command-line will otherwise end up with a polymorphic \verb|False| inductive which will introduce extra universes when using the \verb|admit| tactic which may break some files.
(This was encountered in the wild; see \url{https://github.com/JasonGross/coq-tools/issues/74} for details.\todo{better urls?})

Second, because the file we're minimizing may make use of \verb|False| and may rely on it being syntactically equal to the version in Coq's standard library \todo{does this need an example?}, we must wrap our \verb|Inductive False| in a module that is not exported to the file.
This means that the \verb|admit| tactic must itself be in a module that is exported.
We could alternatively qualify the version of \verb|False| that we're using, but this would prevent removal of our \verb|Inductive False|, and we would prefer to be able to remove this line when the Coq standard library is available.
The code we now have, after the Ltac support snippet, is:
\\\todo{code moved to \autoref{code:ltac-support:1}}\label{code:ltac-support:1:orig}

Third, and finally, the line \texttt{Ltac admit := abstract case proof\_admitted} errors in Coq 8.4 with ``Error: Reserved Ltac name admit.''
Hence we use \texttt{Tactic Notation "admit" := abstract case proof\_admitted} instead.

Thus the fragment we add for the \verb|admit| tactic is ultimately:
\\\todo{code moved to \autoref{code:ltac-support:2}}\label{code:ltac-support:2:orig}


\section{controlflow}
\begin{enumerate}
\item Repeat the following steps until a fixed point is reached, skipping any steps that change the error message in a forbidden way.
\begin{enumerate}
\item Remove comments, inserting a header comment with metadata about minimization statistics.
\item Insert a further header which defines the \verb|admit| tactic.
  We must define this tactic ourselves to handle files which build with a non-standard standard library (such as HoTT) and hence don't have access to \verb|Coq.Compat.AdmitAxiom|.
  To support multiple versions of Coq, we need to feature-test Coq to see what fragment of code grants us access to Ltac without loading the standard library; this feature-test is done once and for all for a single run of the bug minimizer.
\item Split \verb|Require| statements from \verb|Import|/\verb|Export|, disambiguating the statements at the same time.
  (If we don't disambiguate them, the \verb|Import|s might point to the wrong module.)
\item Move the \verb|Require| statements to the top of the file, transitively closing them (this allows us to remove \verb|Require| statements which are only needed to transitively require other files).
\item Parse the file into statements (with exactly two exceptions, statements in Coq end with \verb|.| or \verb|...| followed by whitespace; the exceptions are \verb|{| and \verb|}|).
  Note that this is complicated by needing to handle strings which contain a full stop followed by whitespace, and by the fact that comments in Coq are well-balanced and nestable and can contain strings which themselves contain comment identifiers without nesting further comments.
\item Remove statements after the line which generates the error.
\item Split the statements into structured blocks, one per definition/proof.
  We should not assume that we can remove lines from the end of a proof without breaking things, so we treat each theorem/lemma/definition block, combined with its proof, as a unit.
  This is made simpler by the fact that Coq strongly discourages nested proofs, and by the fact that Coq generally forces definitions and their bodies to be side-by-side.
  There are a couple of exceptions to this strategy, that have dedicated passes for handling them:
  \begin{itemize}
  \item \verb|Program| \verb|Definition|s, \verb|Fixpoint|s, etc, have \verb|Obligation| blocks which can be interwoven with other definitions.
    The solution here is to replace such \verb|Obligation| blocks with \verb|Admit Obligations|, which is thankfully a no-op when no obligations are present.
    \todo{Insert some code showing how minimization of program definitions proceeds with admit obligations}
  \item Sections, modules, and module types have paired vernacular delimiters.
    Currently there is some code to eliminate empty blocks, but it doesn't work very well, and misses many cases.
    There's an open feature request for a \verb|Try| vernacular command at https://github.com/coq/coq/issues/15051, which would solve the issue of coupled commands in general.
  \end{itemize}
\item Repeat the following steps until a fixed point is reached (order presented here is not faithful to the minimizer):
  \begin{itemize}
  \item Remove each structured block (whose removal does not change the error message), one at a time, from the end of the file to the beginning of the file.
  \item Replace proof scripts with \verb|Admitted|.
    (There's some nuance about the best ordering strategy to try here, whether to first try all the \verb|Qed| lemmas, whether to first try all lemmas and definitions at once, or whether to just go one at a time in reverse order.)
    Also, we actually replace them with \verb|admit. Defined.| so that previously unfoldable constants remain unfoldable.
    Technically we should try \verb|Admitted| when the proof script previously ended with \verb|Qed| and replacing with \verb|admit. Defined.| fails, because it might be the case that some later tactic relies on this definition not being unfoldable, but we have not encountered such a case yet, and it seems quite unlikely.
  \item Replace \verb|abstract tac| with \verb|admit|, potentially simplifying proof scripts and decreasing dependencies.
  \item Split \verb|Definition foo args : ty := body.| into \verb|Definition foo args : ty.| and \verb|Proof. refine (body). Defined.| so that such definitions can potentially be admitted later.
  \item Replace \verb|Module Foo| with \verb|Module Export Foo|, potentially allowing the removal of \verb|Import| statements later, and potentially eventually allowing the removal of the module itself.
  \item Split \verb|Import| and \verb|Export| statements containing multiple modules into separate statements, so they can be removed separately.
  \item Early on, some likely-to-succeed steps are tried, such as removing tactics, \verb|Variable| and \verb|Context| statements, and definitions which are not referred to at all after their definition.
    This step is superseded by removing each and every structured block one at a time, but may result in faster minimization.
  \end{itemize}
\end{enumerate}
\end{enumerate}




\end{document}
